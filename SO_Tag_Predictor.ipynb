{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rAZDkLNPU93Y"
   },
   "source": [
    "# Stack Overflow: Tag Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpVk_usWU93a"
   },
   "source": [
    "<h1>1. Business Problem </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-1WlUboU93c"
   },
   "source": [
    "<h2> 1.1 Description </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37SdsKLdU93e"
   },
   "source": [
    "<p style='font-size:18px'><b> Description </b></p>\n",
    "<p>\n",
    "Stack Overflow is the largest, most trusted online community for developers to learn, share their programming knowledge, and build their careers.<br />\n",
    "<br />\n",
    "Stack Overflow is something which every programmer use one way or another. Each month, over 50 million developers come to Stack Overflow to learn, share their knowledge, and build their careers. It features questions and answers on a wide range of topics in computer programming. The website serves as a platform for users to ask and answer questions, and, through membership and active participation, to vote questions and answers up or down and edit questions and answers in a fashion similar to a wiki or Digg. As of April 2014 Stack Overflow has over 4,000,000 registered users, and it exceeded 10,000,000 questions in late August 2015. Based on the type of tags assigned to questions, the top eight most discussed topics on the site are: Java, JavaScript, C#, PHP, Android, jQuery, Python and HTML.<br />\n",
    "<br />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9brAMjUsU93f"
   },
   "source": [
    "<p style='font-size:18px'><b> Problem Statemtent </b></p>\n",
    "Suggest the tags based on the content that was there in the question posted on Stackoverflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URJINSY2U93h"
   },
   "source": [
    "<p style='font-size:18px'><b> Source:  </b> https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggC_3T9XU93j"
   },
   "source": [
    "<h2> 1.2 Source / useful links </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr7T9iknU93l"
   },
   "source": [
    "Data Source : https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/data <br>\n",
    "Youtube : https://youtu.be/nNDqbUhtIRg <br>\n",
    "Research paper : https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tagging-1.pdf <br>\n",
    "Research paper : https://dl.acm.org/citation.cfm?id=2660970&dl=ACM&coll=DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbXm8lAuU93p"
   },
   "source": [
    "<h2> 1.3 Real World / Business Objectives and Constraints </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88TwKPItU93q"
   },
   "source": [
    "1. Predict as many tags as possible with high precision and recall.\n",
    "2. Incorrect tags could impact customer experience on StackOverflow.\n",
    "3. No strict latency constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lE0wX1roU93s"
   },
   "source": [
    "<h1>2. Machine Learning problem </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynAszp_uU93u"
   },
   "source": [
    "<h2> 2.1 Data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elwijxMGU93w"
   },
   "source": [
    "<h3> 2.1.1 Data Overview </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdFiIj7_U93x"
   },
   "source": [
    "Refer: https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/data\n",
    "<br>\n",
    "All of the data is in 2 files: Train and Test.<br />\n",
    "<pre>\n",
    "<b>Train.csv</b> contains 4 columns: Id,Title,Body,Tags.<br />\n",
    "<b>Test.csv</b> contains the same columns but without the Tags, which you are to predict.<br />\n",
    "<b>Size of Train.csv</b> - 6.75GB<br />\n",
    "<b>Size of Test.csv</b> - 2GB<br />\n",
    "<b>Number of rows in Train.csv</b> = 6034195<br />\n",
    "</pre>\n",
    "The questions are randomized and contains a mix of verbose text sites as well as sites related to math and programming. The number of questions from each site may vary, and no filtering has been performed on the questions (such as closed questions).<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ji0A66hWU93z"
   },
   "source": [
    "__Data Field Explaination__\n",
    "\n",
    "Dataset contains 6,034,195 rows. The columns in the table are:<br />\n",
    "<pre>\n",
    "<b>Id</b> - Unique identifier for each question<br />\n",
    "<b>Title</b> - The question's title<br />\n",
    "<b>Body</b> - The body of the question<br />\n",
    "<b>Tags</b> - The tags associated with the question in a space-seperated format (all lowercase, should not contain tabs '\\t' or ampersands '&')<br />\n",
    "</pre>\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNDiy42GU931"
   },
   "source": [
    "<h3>2.1.2 Example Data point </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "D5IcxRbYU932"
   },
   "source": [
    "<pre>\n",
    "<b>Title</b>:  Implementing Boundary Value Analysis of Software Testing in a C++ program?\n",
    "<b>Body </b>: <pre><code>\n",
    "        #include&lt;\n",
    "        iostream&gt;\\n\n",
    "        #include&lt;\n",
    "        stdlib.h&gt;\\n\\n\n",
    "        using namespace std;\\n\\n\n",
    "        int main()\\n\n",
    "        {\\n\n",
    "                 int n,a[n],x,c,u[n],m[n],e[n][4];\\n         \n",
    "                 cout&lt;&lt;\"Enter the number of variables\";\\n         cin&gt;&gt;n;\\n\\n         \n",
    "                 cout&lt;&lt;\"Enter the Lower, and Upper Limits of the variables\";\\n         \n",
    "                 for(int y=1; y&lt;n+1; y++)\\n         \n",
    "                 {\\n                 \n",
    "                    cin&gt;&gt;m[y];\\n                 \n",
    "                    cin&gt;&gt;u[y];\\n         \n",
    "                 }\\n         \n",
    "                 for(x=1; x&lt;n+1; x++)\\n         \n",
    "                 {\\n                 \n",
    "                    a[x] = (m[x] + u[x])/2;\\n         \n",
    "                 }\\n         \n",
    "                 c=(n*4)-4;\\n         \n",
    "                 for(int a1=1; a1&lt;n+1; a1++)\\n         \n",
    "                 {\\n\\n             \n",
    "                    e[a1][0] = m[a1];\\n             \n",
    "                    e[a1][1] = m[a1]+1;\\n             \n",
    "                    e[a1][2] = u[a1]-1;\\n             \n",
    "                    e[a1][3] = u[a1];\\n         \n",
    "                 }\\n         \n",
    "                 for(int i=1; i&lt;n+1; i++)\\n         \n",
    "                 {\\n            \n",
    "                    for(int l=1; l&lt;=i; l++)\\n            \n",
    "                    {\\n                 \n",
    "                        if(l!=1)\\n                 \n",
    "                        {\\n                    \n",
    "                            cout&lt;&lt;a[l]&lt;&lt;\"\\\\t\";\\n                 \n",
    "                        }\\n            \n",
    "                    }\\n            \n",
    "                    for(int j=0; j&lt;4; j++)\\n            \n",
    "                    {\\n                \n",
    "                        cout&lt;&lt;e[i][j];\\n                \n",
    "                        for(int k=0; k&lt;n-(i+1); k++)\\n                \n",
    "                        {\\n                    \n",
    "                            cout&lt;&lt;a[k]&lt;&lt;\"\\\\t\";\\n               \n",
    "                        }\\n                \n",
    "                        cout&lt;&lt;\"\\\\n\";\\n            \n",
    "                    }\\n        \n",
    "                 }    \\n\\n        \n",
    "                 system(\"PAUSE\");\\n        \n",
    "                 return 0;    \\n\n",
    "        }\\n\n",
    "        </code></pre>\\n\\n\n",
    "        <p>The answer should come in the form of a table like</p>\\n\\n\n",
    "        <pre><code>       \n",
    "        1            50              50\\n       \n",
    "        2            50              50\\n       \n",
    "        99           50              50\\n       \n",
    "        100          50              50\\n       \n",
    "        50           1               50\\n       \n",
    "        50           2               50\\n       \n",
    "        50           99              50\\n       \n",
    "        50           100             50\\n       \n",
    "        50           50              1\\n       \n",
    "        50           50              2\\n       \n",
    "        50           50              99\\n       \n",
    "        50           50              100\\n\n",
    "        </code></pre>\\n\\n\n",
    "        <p>if the no of inputs is 3 and their ranges are\\n\n",
    "        1,100\\n\n",
    "        1,100\\n\n",
    "        1,100\\n\n",
    "        (could be varied too)</p>\\n\\n\n",
    "        <p>The output is not coming,can anyone correct the code or tell me what\\'s wrong?</p>\\n'\n",
    "<b>Tags </b>: 'c++ c'\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MomJjFPnU934"
   },
   "source": [
    "<h2>2.2 Mapping the real-world problem to a Machine Learning Problem </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJ5lyxIUU936"
   },
   "source": [
    "<h3> 2.2.1 Type of Machine Learning Problem </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Jb9u-38U938"
   },
   "source": [
    "<p> It is a multi-label classification problem  <br>\n",
    "<b>Multi-label Classification</b>: Multilabel classification assigns to each sample a set of target labels. This can be thought as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A question on Stackoverflow might be about any of C, Pointers, FileIO and/or memory-management at the same time or none of these. <br>\n",
    "__Credit__: http://scikit-learn.org/stable/modules/multiclass.html\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIOwycKkU93_"
   },
   "source": [
    "<h3>2.2.2 Performance metric </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDQXZ4k5U94A"
   },
   "source": [
    "<b>Micro-Averaged F1-Score (Mean F Score) </b>: \n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "<i>F1 = 2 * (precision * recall) / (precision + recall)</i><br>\n",
    "\n",
    "In the multi-class and multi-label case, this is the weighted average of the F1 score of each class. <br>\n",
    "\n",
    "<b>'Micro f1 score': </b><br>\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives. This is a better metric when we have class imbalance.\n",
    "<br>\n",
    "\n",
    "<b>'Macro f1 score': </b><br>\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "<br>\n",
    "\n",
    "https://www.kaggle.com/wiki/MeanFScore <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html <br>\n",
    "<br>\n",
    "<b> Hamming loss </b>: The Hamming loss is the fraction of labels that are incorrectly predicted. <br>\n",
    "https://www.kaggle.com/wiki/HammingLoss <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xqsVRjSmU94C"
   },
   "source": [
    "<h1> 3. Exploratory Data Analysis </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azda-BmRU94H"
   },
   "source": [
    "<h2> 3.1 Data Loading and Cleaning </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import os\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import datetime as dt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from skmultilearn.adapt import mlknn\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62GDC_VjU94J"
   },
   "source": [
    "<h3>3.1.1 Using Pandas with SQLite to Load the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "289tS71cU94L"
   },
   "outputs": [],
   "source": [
    "#Creating db file from csv\n",
    "#Learn SQL: https://www.w3schools.com/sql/default.asp\n",
    "if not os.path.isfile('train.db'):\n",
    "    start = datetime.now()\n",
    "    disk_engine = create_engine('sqlite:///train.db')\n",
    "    start = dt.datetime.now()\n",
    "    chunksize = 180000\n",
    "    j = 0\n",
    "    index_start = 1\n",
    "    for df in pd.read_csv('Train.csv', names=['Id', 'Title', 'Body', 'Tags'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n",
    "        df.index += index_start\n",
    "        j+=1\n",
    "        print('{} rows'.format(j*chunksize))\n",
    "        df.to_sql('data', disk_engine, if_exists='append')\n",
    "        index_start = df.index[-1] + 1\n",
    "    print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5yUhXVNU94Q"
   },
   "source": [
    "<h3> 3.1.2 Counting the number of rows </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3ORCclXYU94R",
    "outputId": "9625d8e0-bf34-413e-8246-932fa1cb21b7"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('train.db'):\n",
    "    start = datetime.now()\n",
    "    con = sqlite3.connect('train.db')\n",
    "    num_rows = pd.read_sql_query(\"\"\"SELECT count(*) FROM data\"\"\", con)\n",
    "    #Always remember to close the database\n",
    "    print(\"Number of rows in the database :\",\"\\n\",num_rows['count(*)'].values[0])\n",
    "    con.close()\n",
    "    print(\"Time taken to count the number of rows :\", datetime.now() - start)\n",
    "else:\n",
    "    print(\"Please download the train.db file from drive or run the above cell to genarate train.db file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xso2eOEvU94Z"
   },
   "source": [
    "<h3>3.1.3 Checking for duplicates </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iBHCcr3DU94b",
    "outputId": "2340d414-8570-4d7e-fda6-a69c39f13780"
   },
   "outputs": [],
   "source": [
    "#Learn SQl: https://www.w3schools.com/sql/default.asp\n",
    "if os.path.isfile('train.db'):\n",
    "    start = datetime.now()\n",
    "    con = sqlite3.connect('train.db')\n",
    "    df_no_dup = pd.read_sql_query('SELECT Title, Body, Tags, COUNT(*) as cnt_dup FROM data GROUP BY Title, Body, Tags', con)\n",
    "    con.close()\n",
    "    print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "else:\n",
    "    print(\"Please download the train.db file from drive or run the first to genarate train.db file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Gap4NRPWU94h",
    "outputId": "e0af72d7-4faf-4232-849b-d27bc1963221"
   },
   "outputs": [],
   "source": [
    "df_no_dup.head()\n",
    "# we can observe that there are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JzFO4EeDU94n",
    "outputId": "198980b2-6480-4a49-ab0a-8074199f5edb"
   },
   "outputs": [],
   "source": [
    "print(\"number of duplicate questions :\", num_rows['count(*)'].values[0]- df_no_dup.shape[0], \"(\",(1-((df_no_dup.shape[0])/(num_rows['count(*)'].values[0])))*100,\"% )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gd2VdpN6U94t",
    "outputId": "05af061e-31e0-4a93-d41a-d738c5b9e890"
   },
   "outputs": [],
   "source": [
    "# number of times each question appeared in our database\n",
    "df_no_dup.cnt_dup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are null values in the column Tag\n",
    "\n",
    "# How to check for if there are null values in any columns\n",
    "print(df_no_dup.isnull().any(axis=0))\n",
    "(df_no_dup.isnull().any(axis=0))\n",
    "\n",
    "#How to get all the rows that have null values\n",
    "df_no_dup[df_no_dup.isnull().any(axis=1)]\n",
    "\n",
    "#How to drop the rows that contain null values\n",
    "df_no_dup = df_no_dup[~df_no_dup.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EogeNAhCU94z",
    "outputId": "661010f0-0d38-4a33-d20f-e94ef51344ab"
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "df_no_dup[\"tag_count\"] = df_no_dup[\"Tags\"].apply(lambda text: len(text.split(\" \")))\n",
    "# adding a new feature number of tags per question\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "df_no_dup.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iItMHo6MU948",
    "outputId": "824508df-c0c0-4d33-a667-83af84360bfc"
   },
   "outputs": [],
   "source": [
    "# distribution of number of tags per question\n",
    "df_no_dup.tag_count.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2xMgCGUKU95C"
   },
   "outputs": [],
   "source": [
    "#Creating a new database with no duplicates\n",
    "if not os.path.isfile('train_no_dup.db'):\n",
    "    disk_dup = create_engine(\"sqlite:///train_no_dup.db\")\n",
    "    no_dup = pd.DataFrame(df_no_dup, columns=['Title', 'Body', 'Tags'])\n",
    "    no_dup.to_sql('no_dup_train',disk_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6Ou53MzeU95H",
    "outputId": "0b643de8-3481-45d0-ff2e-8400f7f5f12b"
   },
   "outputs": [],
   "source": [
    "#This method seems more appropriate to work with this much data.\n",
    "#creating the connection with database file.\n",
    "if os.path.isfile('train_no_dup.db'):\n",
    "    start = datetime.now()\n",
    "    con = sqlite3.connect('train_no_dup.db')\n",
    "    tag_data = pd.read_sql_query(\"\"\"SELECT Tags FROM no_dup_train\"\"\", con)\n",
    "    #Always remember to close the database\n",
    "    con.close()\n",
    "\n",
    "    # Let's now drop unwanted column.\n",
    "    tag_data.drop(tag_data.index[0], inplace=True)\n",
    "    #Printing first 5 columns from our data frame\n",
    "    tag_data.head()\n",
    "    print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "else:\n",
    "    print(\"Please download the train.db file from drive or run the above cells to genarate train.db file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwZVL3doU95O"
   },
   "source": [
    "<h2> 3.2 Analysis of Tags </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYs5peW8U95P"
   },
   "source": [
    "<h3> 3.2.1 Total number of unique tags </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ROdC95M_U95Q"
   },
   "outputs": [],
   "source": [
    "# Importing & Initializing the \"CountVectorizer\" object, which \n",
    "#is scikit-learn's bag of words tool.\n",
    "\n",
    "#by default 'split()' will tokenize each tag using space.\n",
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split())\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of strings.\n",
    "tag_dtm = vectorizer.fit_transform(tag_data['Tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Oz5N0GH0U95V",
    "outputId": "a65cd48f-6df2-44f5-e77c-99b38bb7ae2a"
   },
   "outputs": [],
   "source": [
    "print(\"Number of data points :\", tag_dtm.shape[0])\n",
    "print(\"Number of unique tags :\", tag_dtm.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Otn6CUuQU95b",
    "outputId": "91ff8093-ebae-4218-a786-a117a9eadc0e"
   },
   "outputs": [],
   "source": [
    "#'get_feature_name()' gives us the vocabulary.\n",
    "tags = vectorizer.get_feature_names()\n",
    "#Lets look at the tags we have.\n",
    "print(\"Some of the tags we have :\", tags[:10])\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(tag_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQa3ETSeU95g"
   },
   "source": [
    "<h3> 3.2.3 Number of times a tag appeared </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "beThyuyqU95h"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/15115765/how-to-access-sparse-matrix-elements\n",
    "#Lets now store the document term matrix in a dictionary.\n",
    "freqs = tag_dtm.sum(axis=0).A1\n",
    "result = dict(zip(tags, freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result['adobe-photoshop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6ALwSSx9U95m",
    "outputId": "452a449b-5605-4c32-e571-961eee563161"
   },
   "outputs": [],
   "source": [
    "#Saving this dictionary to csv files.\n",
    "if not os.path.isfile('tag_counts_dict_dtm.csv'):\n",
    "    with open('tag_counts_dict_dtm.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in result.items():\n",
    "            writer.writerow([key, value])\n",
    "tag_df = pd.read_csv(\"tag_counts_dict_dtm.csv\", names=['Tags', 'Counts'])\n",
    "tag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mzmS8yiNU95t"
   },
   "outputs": [],
   "source": [
    "tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\n",
    "tag_counts = tag_df_sorted['Counts'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6mcj55FIU95z",
    "outputId": "d6c9d567-a9a4-48e6-9d44-6f099f198339"
   },
   "outputs": [],
   "source": [
    "plt.plot(tag_counts)\n",
    "plt.title(\"Distribution of number of times tag appeared questions\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UzTqln6XU955",
    "outputId": "07f08ef0-d26b-4797-eb3c-6586011e2588"
   },
   "outputs": [],
   "source": [
    "plt.plot(tag_counts[0:10000])\n",
    "plt.title('first 10k tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()\n",
    "print(len(tag_counts[0:10000:25]), tag_counts[0:10000:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ntm8E_K9U95-",
    "outputId": "750edd11-e52b-46bd-caba-1835994a0e05"
   },
   "outputs": [],
   "source": [
    "plt.plot(tag_counts[0:1000])\n",
    "plt.title('first 1k tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()\n",
    "print(len(tag_counts[0:1000:5]), tag_counts[0:1000:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-SRUeKuWU96I",
    "outputId": "f20a242c-2f7d-4e7d-f3f2-291d2ea38caa"
   },
   "outputs": [],
   "source": [
    "plt.plot(tag_counts[0:500])\n",
    "plt.title('first 500 tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()\n",
    "print(len(tag_counts[0:500:5]), tag_counts[0:500:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CCOE0Nu4U96N",
    "outputId": "1f8df41e-b0dc-4d7f-a3b2-0a0c472ef448"
   },
   "outputs": [],
   "source": [
    "plt.plot(tag_counts[0:100], c='b')\n",
    "plt.scatter(x=list(range(0,100,5)), y=tag_counts[0:100:5], c='orange', label=\"quantiles with 0.05 intervals\")\n",
    "# quantiles with 0.25 difference\n",
    "plt.scatter(x=list(range(0,100,25)), y=tag_counts[0:100:25], c='m', label = \"quantiles with 0.25 intervals\")\n",
    "\n",
    "for x,y in zip(list(range(0,100,25)), tag_counts[0:100:25]):\n",
    "    plt.annotate(s=\"({} , {})\".format(x,y), xy=(x,y), xytext=(x-0.05, y+500))\n",
    "\n",
    "plt.title('first 100 tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(len(tag_counts[0:100:5]), tag_counts[0:100:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MnwXytypU96R",
    "outputId": "3427f2fd-4b27-435e-d1a8-f1bcf4515d9c"
   },
   "outputs": [],
   "source": [
    "# Store tags greater than 10K in one list\n",
    "lst_tags_gt_10k = tag_df[tag_df.Counts>10000].Tags\n",
    "#Print the length of the list\n",
    "print ('{} Tags are used more than 10000 times'.format(len(lst_tags_gt_10k)))\n",
    "# Store tags greater than 100K in one list\n",
    "lst_tags_gt_100k = tag_df[tag_df.Counts>100000].Tags\n",
    "#Print the length of the list.\n",
    "print ('{} Tags are used more than 100000 times'.format(len(lst_tags_gt_100k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oOXPF7q1U96W"
   },
   "source": [
    "<b>Observations:</b><br />\n",
    "1. There are total 153 tags which are used more than 10000 times.\n",
    "2. 14 tags are used more than 100000 times.\n",
    "3. Most frequent tag (i.e. c#) is used 331505 times.\n",
    "4. Since some tags occur much more frequenctly than others, Micro-averaged F1-score is the appropriate metric for this probelm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zb-Tg_sgU96a"
   },
   "source": [
    "<h3> 3.2.4 Tags Per Question </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bsPStbjGU96g",
    "outputId": "2e57d47f-1ae9-4776-ab17-d4315b9d0c14"
   },
   "outputs": [],
   "source": [
    "#Storing the count of tag in each question in list 'tag_count'\n",
    "tag_quest_count = tag_dtm.sum(axis=1).tolist()\n",
    "#Converting list of lists into single list, we will get [[3], [4], [2], [2], [3]] and we are converting this to [3, 4, 2, 2, 3]\n",
    "tag_quest_count=[int(j) for i in tag_quest_count for j in i]\n",
    "print ('We have total {} datapoints.'.format(len(tag_quest_count)))\n",
    "\n",
    "print(tag_quest_count[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DLbm7crfU96n",
    "outputId": "f71ff030-9937-4081-9d4a-fb2c62386e11"
   },
   "outputs": [],
   "source": [
    "print( \"Maximum number of tags per question: %d\"%max(tag_quest_count))\n",
    "print( \"Minimum number of tags per question: %d\"%min(tag_quest_count))\n",
    "print( \"Avg. number of tags per question: %f\"% ((sum(tag_quest_count)*1.0)/len(tag_quest_count)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mb1vdd8KU96x",
    "outputId": "4579b7c7-ff27-4fb3-f3b9-29401610a3a0"
   },
   "outputs": [],
   "source": [
    "sns.countplot(tag_quest_count, palette='gist_rainbow')\n",
    "plt.title(\"Number of tags in the questions \")\n",
    "plt.xlabel(\"Number of Tags\")\n",
    "plt.ylabel(\"Number of questions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0RsUcQkNU963"
   },
   "source": [
    "<b>Observations:</b><br />\n",
    "1. Maximum number of tags per question: 5\n",
    "2. Minimum number of tags per question: 1\n",
    "3. Avg. number of tags per question: 2.899\n",
    "4. Most of the questions are having 2 or 3 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=result.items()\n",
    "c=dict(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-M5-A-MU963"
   },
   "source": [
    "<h3>3.2.5 Most Frequent Tags </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Brokf0gSU965",
    "outputId": "b9794ec6-8e67-49ad-f641-069638dbd951"
   },
   "outputs": [],
   "source": [
    "# Ploting word cloud\n",
    "start = datetime.now()\n",
    "\n",
    "# Lets first convert the 'result' dictionary to 'list of tuples'\n",
    "tup = dict(result.items())\n",
    "\n",
    "#Initializing WordCloud using frequencies of tags.\n",
    "wordcloud = WordCloud(    background_color='black',\n",
    "                          width=1600,\n",
    "                          height=800,\n",
    "                    ).generate_from_frequencies(tup)\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "fig.savefig(\"tag.png\")\n",
    "plt.show()\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_n7RaKj2U96-"
   },
   "source": [
    "<b>Observations:</b><br />\n",
    "A look at the word cloud shows that \"c#\", \"java\", \"php\", \"asp.net\", \"javascript\", \"c++\" are some of the most frequent tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hpc8IWjqU96_"
   },
   "source": [
    "<h3> 3.2.6 The top 20 tags </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ov3WmIEHU97A",
    "outputId": "8e4867c3-81da-445d-a7b3-fc8d4c283be0"
   },
   "outputs": [],
   "source": [
    "i=np.arange(30)\n",
    "tag_df_sorted.head(30).plot(kind='bar')\n",
    "plt.title('Frequency of top 20 tags')\n",
    "#This maps the index to word like C#\n",
    "plt.xticks(i, tag_df_sorted['Tags'])\n",
    "plt.xlabel('Tags')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlmizz7LU97D"
   },
   "source": [
    "<b>Observations:</b><br />\n",
    "1. Majority of the most frequent tags are programming language.\n",
    "2. C# is the top most frequent programming language.\n",
    "3. Android, IOS, Linux and windows are among the top most frequent operating systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-Z7F0_mU97F"
   },
   "source": [
    "<h3> 3.3 Cleaning and preprocessing of Questions </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cWzF-nN6U97G"
   },
   "source": [
    "<h3> 3.3.1 Preprocessing </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeR1aoQaU97H"
   },
   "source": [
    "<ol> \n",
    "    <li> Sample 1M data points </li>\n",
    "    <li> Separate out code-snippets from Body </li>\n",
    "    <li> Remove Spcial characters from Question title and description (not in code)</li>\n",
    "    <li> Remove stop words (Except 'C') </li>\n",
    "    <li> Remove HTML Tags </li>\n",
    "    <li> Convert all the characters into small letters </li>\n",
    "    <li> Use SnowballStemmer to stem the words </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Qr2xhpsAU97I"
   },
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(data))\n",
    "    return cleantext\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LCDUa4KxU97L",
    "outputId": "8270b10f-cf17-4025-9440-5f2b2b62579c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the databse:\n",
      "QuestionsProcessed\n"
     ]
    }
   ],
   "source": [
    "#http://www.sqlitetutorial.net/sqlite-python/create-tables/\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None\n",
    "\n",
    "def create_table(conn, create_table_sql):\n",
    "    \"\"\" create a table from the create_table_sql statement\n",
    "    :param conn: Connection object\n",
    "    :param create_table_sql: a CREATE TABLE statement\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "def checkTableExists(dbcon):\n",
    "    cursr = dbcon.cursor()\n",
    "    str = \"select name from sqlite_master where type='table'\"\n",
    "    table_names = cursr.execute(str)\n",
    "    print(\"Tables in the databse:\")\n",
    "    tables =table_names.fetchall() \n",
    "    print(tables[0][0])\n",
    "    return(len(tables))\n",
    "\n",
    "def create_database_table(database, query):\n",
    "    conn = create_connection(database)\n",
    "    if conn is not None:\n",
    "        create_table(conn, query)\n",
    "        checkTableExists(conn)\n",
    "    else:\n",
    "        print(\"Error! cannot create the database connection.\")\n",
    "    conn.close()\n",
    "\n",
    "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS QuestionsProcessed (question text NOT NULL, code text, tags text, words_pre integer, words_post integer, is_code integer);\"\"\"\n",
    "create_database_table(\"Processed.db\", sql_create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qz092AdFU97P",
    "outputId": "b00a3eec-60d8-4c08-83c8-54debd0a4123"
   },
   "outputs": [],
   "source": [
    "# http://www.sqlitetutorial.net/sqlite-delete/\n",
    "# https://stackoverflow.com/questions/2279706/select-random-row-from-a-sqlite-table\n",
    "start = datetime.now()\n",
    "read_db = 'train_no_dup.db'\n",
    "write_db = 'Processed.db'\n",
    "if os.path.isfile(read_db):\n",
    "    conn_r = create_connection(read_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        reader.execute(\"SELECT Title, Body, Tags From no_dup_train ORDER BY RANDOM() LIMIT 1000000;\")\n",
    "\n",
    "if os.path.isfile(write_db):\n",
    "    conn_w = create_connection(write_db)\n",
    "    if conn_w is not None:\n",
    "        tables = checkTableExists(conn_w)\n",
    "        writer =conn_w.cursor()\n",
    "        if tables != 0:\n",
    "            writer.execute(\"DELETE FROM QuestionsProcessed WHERE 1\")\n",
    "            print(\"Cleared All the rows\")\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FELLR5FBU97U"
   },
   "source": [
    "__ we create a new data base to store the sampled and preprocessed questions __\n",
    "### Don't run this cell, it takes quite some time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "clKVIuAcU97W",
    "outputId": "eab75c6e-f76e-4cf2-eb4d-c589a9908be5"
   },
   "outputs": [],
   "source": [
    "#http://www.bernzilla.com/2008/05/13/selecting-a-random-row-from-an-sqlite-table/\n",
    "\n",
    "start = datetime.now()\n",
    "preprocessed_data_list=[]\n",
    "reader.fetchone()\n",
    "questions_with_code=0\n",
    "len_pre=0\n",
    "len_post=0\n",
    "questions_proccesed = 0\n",
    "for row in reader:\n",
    "\n",
    "    is_code = 0\n",
    "    title, question, tags = row[0], row[1], row[2]\n",
    "\n",
    "    if '<code>' in question:\n",
    "        questions_with_code+=1\n",
    "        is_code = 1\n",
    "    x = len(question)+len(title)\n",
    "    len_pre+=x\n",
    "\n",
    "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
    "\n",
    "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
    "    question=striphtml(question.encode('utf-8'))\n",
    "\n",
    "    title=title.encode('utf-8')\n",
    "\n",
    "    question=str(title)+\" \"+str(question)\n",
    "    question=re.sub(r'[^A-Za-z]+',' ',question)\n",
    "    words=word_tokenize(str(question.lower()))\n",
    "\n",
    "    #Removing all single letter and and stopwords from question exceptt for the letter 'c'\n",
    "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
    "\n",
    "    len_post+=len(question)\n",
    "    tup = (question,code,tags,x,len(question),is_code)\n",
    "    questions_proccesed += 1\n",
    "    writer.execute(\"insert into QuestionsProcessed(question,code,tags,words_pre,words_post,is_code) values (?,?,?,?,?,?)\",tup)\n",
    "    if (questions_proccesed%100000==0):\n",
    "        print(\"number of questions completed=\",questions_proccesed)\n",
    "\n",
    "no_dup_avg_len_pre=(len_pre*1.0)/questions_proccesed\n",
    "no_dup_avg_len_post=(len_post*1.0)/questions_proccesed\n",
    "\n",
    "print( \"Avg. length of questions(Title+Body) before processing: %d\"%no_dup_avg_len_pre)\n",
    "print( \"Avg. length of questions(Title+Body) after processing: %d\"%no_dup_avg_len_post)\n",
    "print (\"Percent of questions containing code: %d\"%((questions_with_code*100.0)/questions_proccesed))\n",
    "\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fMihWt4uU97b"
   },
   "outputs": [],
   "source": [
    "# dont forget to close the connections, or else you will end up with locks\n",
    "conn_r.commit()\n",
    "conn_w.commit()\n",
    "conn_r.close()\n",
    "conn_w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "y8VzP3hsU97e",
    "outputId": "5efb2876-7d07-438f-aeac-edeecc694f5b"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        reader.execute(\"SELECT question From QuestionsProcessed LIMIT 10\")\n",
    "        print(\"Questions after preprocessed\")\n",
    "        print('='*100)\n",
    "        reader.fetchone()\n",
    "        for row in reader:\n",
    "            print(row)\n",
    "            print('-'*100)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jSJXxeS0U97i"
   },
   "outputs": [],
   "source": [
    "#Taking 1 Million entries to a dataframe.\n",
    "write_db = 'Processed.db'\n",
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed limit 100000\"\"\", conn_r)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ANsenX1EU97l",
    "outputId": "6170e6c6-86e0-48e9-d1c1-190c0cfc15be"
   },
   "outputs": [],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "I6vsCoLOU97r",
    "outputId": "494f221b-f6ed-4aeb-a557-aeb6d209cf38"
   },
   "outputs": [],
   "source": [
    "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
    "print(\"number of dimensions :\", preprocessed_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qB0bL2drU97w"
   },
   "source": [
    "<h1>4. Machine Learning Models </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZ3-VPbqU97w"
   },
   "source": [
    "<h2> 4.1 Converting tags for multilabel problems </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K88oaTD9U97y"
   },
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>X</th><th>y1</th><th>y2</th><th>y3</th><th>y4</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>x1</td><td>0</td><td>1</td><td>1</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>x1</td><td>1</td><td>0</td><td>0</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>x1</td><td>0</td><td>1</td><td>0</td><td>0</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ttr2m-qiU97z"
   },
   "outputs": [],
   "source": [
    "# binary='true' will give a binary vectorizer\n",
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
    "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_VCwyfHxU972"
   },
   "source": [
    "__ We will sample the number of tags instead considering all of them (due to limitation of computing power) __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FtgktWvU973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n"
     ]
    }
   ],
   "source": [
    "def tags_to_choose(n):\n",
    "    t = multilabel_y.sum(axis=0).tolist()[0]\n",
    "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
    "    multilabel_yn=multilabel_y[:,sorted_tags_i[:n]]\n",
    "    return multilabel_yn\n",
    "\n",
    "def questions_explained_fn(n):\n",
    "    multilabel_yn = tags_to_choose(n)\n",
    "    x= multilabel_yn.sum(axis=1)\n",
    "    return (np.count_nonzero(x==0))\n",
    "\n",
    "print(questions_explained_fn(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XxbL2OyqU978"
   },
   "outputs": [],
   "source": [
    "questions_explained = []\n",
    "total_tags=multilabel_y.shape[1]\n",
    "total_qs=preprocessed_data.shape[0]\n",
    "for i in range(500, total_tags, 100):\n",
    "    questions_explained.append(np.round(((total_qs-questions_explained_fn(i))/total_qs)*100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sbX6_QWtU98B",
    "outputId": "7add8021-d595-4e9e-c506-611520b21a9c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(questions_explained)\n",
    "xlabel = list(500+np.array(range(-50,450,50))*50)\n",
    "ax.set_xticklabels(xlabel)\n",
    "plt.xlabel(\"Number of tags\")\n",
    "plt.ylabel(\"Number Questions coverd partially\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# you can choose any number of tags based on your computing power, minimun is 50(it covers 90% of the tags)\n",
    "print(\"with \",5500,\"tags we are covering \",questions_explained[50],\"% of questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "IgJKNRG0U98F",
    "outputId": "b620a1a2-a9c8-43be-b325-a8ff5ec6c0e0"
   },
   "outputs": [],
   "source": [
    "multilabel_yx = tags_to_choose(100)\n",
    "print(\"number of questions that are not covered :\", questions_explained_fn(5500),\"out of \", total_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BTuhd5XMU98L",
    "outputId": "c60da5fb-99c9-4c8b-f417-633ef97eb598"
   },
   "outputs": [],
   "source": [
    "print(\"Number of tags in sample :\", multilabel_y.shape[1])\n",
    "print(\"number of tags taken :\", multilabel_yx.shape[1],\"(\",(multilabel_yx.shape[1]/multilabel_y.shape[1])*100,\"%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxH1ggjxU98R"
   },
   "source": [
    "__ We consider top 15% tags which covers  99% of the questions __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stMfn7tMU98U"
   },
   "source": [
    "<h2>4.2 Split the data into test and train (80:20) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YqeDZrwWU98U"
   },
   "outputs": [],
   "source": [
    "total_size=preprocessed_data.shape[0]\n",
    "train_size=int(0.80*total_size)\n",
    "\n",
    "x_train=preprocessed_data.head(train_size)\n",
    "x_test=preprocessed_data.tail(total_size - train_size)\n",
    "\n",
    "y_train = multilabel_yx[0:train_size,:]\n",
    "y_test = multilabel_yx[train_size:total_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "TgNXo4eJU98X",
    "outputId": "21bef306-6c57-47af-e70a-e501c4225d4d"
   },
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data :\", y_train.shape)\n",
    "print(\"Number of data points in test data :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "664323eyU98a"
   },
   "source": [
    "<h2>4.3 Featurizing data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EnV3O0WFU98b",
    "outputId": "ef87c28c-4224-451e-c762-0baa2086bd70"
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))\n",
    "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
    "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CB01RkDzU98f",
    "outputId": "cbeda5ce-bbc3-4d6a-8324-53962dd50d56"
   },
   "outputs": [],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "L-JQh1bHU98j",
    "outputId": "63863f36-79ad-4726-de7c-c5d728f8902e"
   },
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/\n",
    "#https://stats.stackexchange.com/questions/117796/scikit-multi-label-classification\n",
    "# classifier = LabelPowerset(GaussianNB())\n",
    "\"\"\"\n",
    "from skmultilearn.adapt import MLkNN\n",
    "classifier = MLkNN(k=21)\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test_multilabel)\n",
    "print(accuracy_score(y_test,predictions))\n",
    "print(metrics.f1_score(y_test, predictions, average = 'macro'))\n",
    "print(metrics.f1_score(y_test, predictions, average = 'micro'))\n",
    "print(metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\"\"\"\n",
    "# we are getting memory error because the multilearn package \n",
    "# is trying to convert the data into dense matrix\n",
    "# ---------------------------------------------------------------------------\n",
    "#MemoryError                               Traceback (most recent call last)\n",
    "#<ipython-input-170-f0e7c7f3e0be> in <module>()\n",
    "#----> classifier.fit(x_train_multilabel, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qh-EUIsAU98l"
   },
   "source": [
    "<h2> 4.4 Applying Logistic Regression with OneVsRest Classifier </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2Cp-1EWpU98m",
    "outputId": "6ecf8c51-1828-438f-dd78-4bf262a5efaa"
   },
   "outputs": [],
   "source": [
    "# this will be taking so much time try not to run it, download the lr_with_equal_weight.pkl file and use to predict\n",
    "# This takes about 6-7 hours to run.\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict(x_test_multilabel)\n",
    "\n",
    "print(\"accuracy :\",metrics.accuracy_score(y_test,predictions))\n",
    "print(\"macro f1 score :\",metrics.f1_score(y_test, predictions, average = 'macro'))\n",
    "print(\"micro f1 scoore :\",metrics.f1_score(y_test, predictions, average = 'micro'))\n",
    "print(\"hamming loss :\",metrics.hamming_loss(y_test,predictions))\n",
    "print(\"Precision recall report :\\n\",metrics.classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hCR2BtdxU98s"
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(classifier, 'lr_with_equal_weight.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvjtTBZ6U98y"
   },
   "source": [
    "<h2> 4.5 Modeling with less data points (0.5M data points) and more weight to title and 500 tags only. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "n0QKMrEwU98y",
    "outputId": "3f1295d3-1aac-4cf0-f7a7-fab13cab6d6d"
   },
   "outputs": [],
   "source": [
    "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS QuestionsProcessed (question text NOT NULL, code text, tags text, words_pre integer, words_post integer, is_code integer);\"\"\"\n",
    "create_database_table(\"Titlemoreweight.db\", sql_create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XHLunpYsU982",
    "outputId": "e79456dd-e137-4e91-8171-8379f4c0c286"
   },
   "outputs": [],
   "source": [
    "# http://www.sqlitetutorial.net/sqlite-delete/\n",
    "# https://stackoverflow.com/questions/2279706/select-random-row-from-a-sqlite-table\n",
    "\n",
    "read_db = 'train_no_dup.db'\n",
    "write_db = 'Titlemoreweight.db'\n",
    "train_datasize = 400000\n",
    "if os.path.isfile(read_db):\n",
    "    conn_r = create_connection(read_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        # for selecting first 0.5M rows\n",
    "        reader.execute(\"SELECT Title, Body, Tags From no_dup_train LIMIT 500001;\")\n",
    "        # for selecting random points\n",
    "        #reader.execute(\"SELECT Title, Body, Tags From no_dup_train ORDER BY RANDOM() LIMIT 500001;\")\n",
    "\n",
    "if os.path.isfile(write_db):\n",
    "    conn_w = create_connection(write_db)\n",
    "    if conn_w is not None:\n",
    "        tables = checkTableExists(conn_w)\n",
    "        writer =conn_w.cursor()\n",
    "        if tables != 0:\n",
    "            writer.execute(\"DELETE FROM QuestionsProcessed WHERE 1\")\n",
    "            print(\"Cleared All the rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvi2298wU986"
   },
   "source": [
    "<h3> 4.5.1 Preprocessing of questions </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNhcD1LYU987"
   },
   "source": [
    "<ol> \n",
    "    <li> Separate Code from Body </li>\n",
    "    <li> Remove Spcial characters from Question title and description (not in code)</li>\n",
    "    <li> <b> Give more weightage to title : Add title three times to the question </b> </li>\n",
    "   \n",
    "    <li> Remove stop words (Except 'C') </li>\n",
    "    <li> Remove HTML Tags </li>\n",
    "    <li> Convert all the characters into small letters </li>\n",
    "    <li> Use SnowballStemmer to stem the words </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ifSmL0M-U98-",
    "outputId": "78019f2f-3a06-4b6d-a761-c8cae342ac42"
   },
   "outputs": [],
   "source": [
    "#http://www.bernzilla.com/2008/05/13/selecting-a-random-row-from-an-sqlite-table/\n",
    "start = datetime.now()\n",
    "preprocessed_data_list=[]\n",
    "reader.fetchone()\n",
    "questions_with_code=0\n",
    "len_pre=0\n",
    "len_post=0\n",
    "questions_proccesed = 0\n",
    "for row in reader:\n",
    "    \n",
    "    is_code = 0\n",
    "    \n",
    "    title, question, tags = row[0], row[1], str(row[2])\n",
    "    \n",
    "    if '<code>' in question:\n",
    "        questions_with_code+=1\n",
    "        is_code = 1\n",
    "    x = len(question)+len(title)\n",
    "    len_pre+=x\n",
    "    \n",
    "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
    "    \n",
    "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
    "    question=striphtml(question.encode('utf-8'))\n",
    "    \n",
    "    title=title.encode('utf-8')\n",
    "    \n",
    "    # adding title three time to the data to increase its weight\n",
    "    # add tags string to the training data\n",
    "    \n",
    "    question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
    "    \n",
    "#     if questions_proccesed<=train_datasize:\n",
    "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question+\" \"+str(tags)\n",
    "#     else:\n",
    "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
    "\n",
    "    question=re.sub(r'[^A-Za-z0-9#+.\\-]+',' ',question)\n",
    "    words=word_tokenize(str(question.lower()))\n",
    "    \n",
    "    #Removing all single letter and and stopwords from question exceptt for the letter 'c'\n",
    "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
    "    \n",
    "    len_post+=len(question)\n",
    "    tup = (question,code,tags,x,len(question),is_code)\n",
    "    questions_proccesed += 1\n",
    "    writer.execute(\"insert into QuestionsProcessed(question,code,tags,words_pre,words_post,is_code) values (?,?,?,?,?,?)\",tup)\n",
    "    if (questions_proccesed%100000==0):\n",
    "        print(\"number of questions completed=\",questions_proccesed)\n",
    "\n",
    "no_dup_avg_len_pre=(len_pre*1.0)/questions_proccesed\n",
    "no_dup_avg_len_post=(len_post*1.0)/questions_proccesed\n",
    "\n",
    "print( \"Avg. length of questions(Title+Body) before processing: %d\"%no_dup_avg_len_pre)\n",
    "print( \"Avg. length of questions(Title+Body) after processing: %d\"%no_dup_avg_len_post)\n",
    "print (\"Percent of questions containing code: %d\"%((questions_with_code*100.0)/questions_proccesed))\n",
    "\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "x54WQvZAU99B"
   },
   "outputs": [],
   "source": [
    "# never forget to close the conections or else we will end up with database locks\n",
    "conn_r.commit()\n",
    "conn_w.commit()\n",
    "conn_r.close()\n",
    "conn_w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpN1ZM2bU99F"
   },
   "source": [
    "__ Sample quesitons after preprocessing of data __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ytEecnCtU99H",
    "outputId": "f3c4992d-41e4-4e4a-9ae9-c40bb79dc43a"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        reader.execute(\"SELECT question From QuestionsProcessed LIMIT 10\")\n",
    "        print(\"Questions after preprocessed\")\n",
    "        print('='*100)\n",
    "        reader.fetchone()\n",
    "        for row in reader:\n",
    "            print(row)\n",
    "            print('-'*100)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IspyyegoU99N"
   },
   "source": [
    "__ Saving Preprocessed data to a Database __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "F_x-ETQJU99P"
   },
   "outputs": [],
   "source": [
    "#Taking 0.5 Million entries to a dataframe.\n",
    "write_db = 'Titlemoreweight.db'\n",
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed\"\"\", conn_r)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bc7hwHjBU99U",
    "outputId": "7a90802a-0630-44da-853d-987e6ece8c4b"
   },
   "outputs": [],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Xk9V0azqU99X",
    "outputId": "c767de50-461e-4454-c634-5487aa543b82"
   },
   "outputs": [],
   "source": [
    "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
    "print(\"number of dimensions :\", preprocessed_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUpccCSkU99Z"
   },
   "source": [
    "__ Converting string Tags to multilable output variables __ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SWg_g1lNU99a"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
    "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbtD0Hx8U99c"
   },
   "source": [
    "__ Selecting 500 Tags __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "h_nMDxAIU99d"
   },
   "outputs": [],
   "source": [
    "questions_explained = []\n",
    "total_tags=multilabel_y.shape[1]\n",
    "total_qs=preprocessed_data.shape[0]\n",
    "for i in range(500, total_tags, 100):\n",
    "    questions_explained.append(np.round(((total_qs-questions_explained_fn(i))/total_qs)*100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fggMk2IJU99f",
    "outputId": "7c443492-b0c4-492d-afc2-16c59b8b954e"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(questions_explained)\n",
    "xlabel = list(500+np.array(range(-50,450,50))*50)\n",
    "ax.set_xticklabels(xlabel)\n",
    "plt.xlabel(\"Number of tags\")\n",
    "plt.ylabel(\"Number Questions coverd partially\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# you can choose any number of tags based on your computing power, minimun is 500(it covers 90% of the tags)\n",
    "print(\"with \",5500,\"tags we are covering \",questions_explained[50],\"% of questions\")\n",
    "print(\"with \",500,\"tags we are covering \",questions_explained[0],\"% of questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VuJzfmNrU99i",
    "outputId": "2c34ec8f-ee38-451d-f477-89f880a98b7d"
   },
   "outputs": [],
   "source": [
    "# we will be taking 500 tags\n",
    "multilabel_yx = tags_to_choose(500)\n",
    "print(\"number of questions that are not covered :\", questions_explained_fn(500),\"out of \", total_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WsduwXTeU99k"
   },
   "outputs": [],
   "source": [
    "x_train=preprocessed_data.head(train_datasize)\n",
    "x_test=preprocessed_data.tail(preprocessed_data.shape[0] - 400000)\n",
    "\n",
    "y_train = multilabel_yx[0:train_datasize,:]\n",
    "y_test = multilabel_yx[train_datasize:preprocessed_data.shape[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iZZDSH_VU99m",
    "outputId": "15b74fe3-29ac-40d2-c393-82260f78ce19"
   },
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data :\", y_train.shape)\n",
    "print(\"Number of data points in test data :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDJ2PvnzU99o"
   },
   "source": [
    "<h3> 4.5.2 Featurizing data with TfIdf vectorizer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "530e8tW9U99o",
    "outputId": "a73463fb-694d-44bf-dc16-97c9b161f0a0"
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))\n",
    "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
    "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "r9iDfzXIU99t",
    "outputId": "23c4dfde-72c4-40c3-9339-29e56fc18b59"
   },
   "outputs": [],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kmmnoy4XU99v"
   },
   "source": [
    "<h3> 4.5.3 Applying Logistic Regression with OneVsRest Classifier </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GnHoxl5DU99w",
    "outputId": "fea313da-ed92-469d-d34b-149d3fc5e01e"
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict (x_test_multilabel)\n",
    "\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6qY1LzZPU991",
    "outputId": "3b3750c1-8e13-478c-cb8a-ff91de626236"
   },
   "outputs": [],
   "source": [
    "joblib.dump(classifier, 'lr_with_more_title_weight.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kWQKiSJxU994",
    "outputId": "7728dac3-cda2-495c-8a6f-dd08cdc1f5fc"
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "classifier_2 = OneVsRestClassifier(LogisticRegression(penalty='l1'), n_jobs=-1)\n",
    "classifier_2.fit(x_train_multilabel, y_train)\n",
    "predictions_2 = classifier_2.predict(x_test_multilabel)\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions_2))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions_2))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions_2, average='micro')\n",
    "recall = recall_score(y_test, predictions_2, average='micro')\n",
    "f1 = f1_score(y_test, predictions_2, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions_2, average='macro')\n",
    "recall = recall_score(y_test, predictions_2, average='macro')\n",
    "f1 = f1_score(y_test, predictions_2, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions_2))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cow4e_y4U997"
   },
   "source": [
    "<h1> 5. Assignments </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ry9fz7FmU998"
   },
   "source": [
    "<ol>\n",
    "    <li> Use bag of words upto 4 grams and compute the micro f1 score with Logistic regression(OvR) </li>\n",
    "    <li> Perform hyperparam tuning on alpha (or lambda) for Logistic regression to improve the performance using GridSearch  </li>\n",
    "    <li> Try OneVsRestClassifier  with Linear-SVM (SGDClassifier with loss-hinge)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the 0.5 million records of data\n",
    "write_db = 'Titlemoreweight.db'\n",
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed limit 10000\"\"\", conn_r)\n",
    "conn_r.commit()\n",
    "conn_r.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary='true' will give a binary vectorizer for tags\n",
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
    "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the top n fields for the tags/labels\n",
    "def tags_to_choose(n):\n",
    "    t = multilabel_y.sum(axis=0).tolist()[0]\n",
    "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
    "    multilabel_yn=multilabel_y[:,sorted_tags_i[:n]]\n",
    "    return multilabel_yn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of questions that are not covered : 900 out of  10000\n"
     ]
    }
   ],
   "source": [
    "# we will be taking 500 tags from the function to keep top 500 frequent fields in csr matrix\n",
    "multilabel_yx = tags_to_choose(500)\n",
    "total_qs=multilabel_yx.shape[0]\n",
    "print(\"number of questions that are not covered :\", questions_explained_fn(500),\"out of \", total_qs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on 400000 data points and splitting into train and test sets\n",
    "train_datasize=int(preprocessed_data.shape[0]*0.8)\n",
    "#Getting the data for train and test data splits \n",
    "x_train=preprocessed_data.head(train_datasize)\n",
    "x_test=preprocessed_data.tail(preprocessed_data.shape[0] - train_datasize)\n",
    "\n",
    "y_train = multilabel_yx[0:train_datasize,:]\n",
    "y_test = multilabel_yx[train_datasize:preprocessed_data.shape[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:06.460069\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=20000, smooth_idf=True, norm=\"l2\", \\\n",
    "                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,4))\n",
    "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
    "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of train data X: (8000, 20000) Y : (8000, 500)\n",
      "Dimensions of test data X: (2000, 20000) Y: (2000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.178\n",
      "Hamming loss  0.003168\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3071, Recall: 0.1784, F1-measure: 0.2096\n",
      "Micro-average quality numbers\n",
      "Precision: 0.6502, Recall: 0.2919, F1-measure: 0.4029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.29      0.37        97\n",
      "           1       0.72      0.44      0.55       140\n",
      "           2       0.60      0.34      0.43       125\n",
      "           3       0.82      0.62      0.70        91\n",
      "           4       0.77      0.50      0.61        88\n",
      "           5       0.76      0.46      0.57       136\n",
      "           6       0.21      0.12      0.15        25\n",
      "           7       0.83      0.37      0.51        27\n",
      "           8       0.88      0.52      0.66        88\n",
      "           9       0.86      0.44      0.59        81\n",
      "          10       0.77      0.44      0.56        61\n",
      "          11       0.74      0.73      0.74        82\n",
      "          12       0.47      0.23      0.31        40\n",
      "          13       0.00      0.00      0.00        34\n",
      "          14       0.50      0.12      0.20        89\n",
      "          15       0.35      0.17      0.23        46\n",
      "          16       0.69      0.51      0.59        39\n",
      "          17       0.38      0.27      0.32        22\n",
      "          18       0.67      0.25      0.36        57\n",
      "          19       0.53      0.22      0.31        45\n",
      "          20       0.43      0.29      0.34        21\n",
      "          21       0.67      0.61      0.64        56\n",
      "          22       0.67      0.44      0.53        18\n",
      "          23       0.64      0.30      0.41        30\n",
      "          24       0.67      0.36      0.47        11\n",
      "          25       0.33      0.14      0.20        29\n",
      "          26       0.20      0.11      0.14         9\n",
      "          27       0.59      0.26      0.36        39\n",
      "          28       0.43      0.33      0.38         9\n",
      "          29       0.73      0.69      0.71        16\n",
      "          30       0.88      0.30      0.45        23\n",
      "          31       0.67      0.21      0.32        29\n",
      "          32       0.85      0.52      0.65        21\n",
      "          33       0.00      0.00      0.00         6\n",
      "          34       0.88      0.78      0.82        45\n",
      "          35       0.25      0.07      0.11        29\n",
      "          36       0.80      0.40      0.53        10\n",
      "          37       0.50      0.12      0.20         8\n",
      "          38       0.60      0.38      0.46         8\n",
      "          39       0.00      0.00      0.00        17\n",
      "          40       1.00      0.09      0.17        11\n",
      "          41       0.50      0.22      0.31         9\n",
      "          42       0.33      0.14      0.20         7\n",
      "          43       0.60      0.75      0.67         4\n",
      "          44       1.00      0.29      0.44         7\n",
      "          45       0.33      0.14      0.20         7\n",
      "          46       0.67      0.29      0.40        28\n",
      "          47       0.00      0.00      0.00         2\n",
      "          48       0.50      0.14      0.22         7\n",
      "          49       0.25      0.14      0.18         7\n",
      "          50       1.00      0.50      0.67        10\n",
      "          51       0.44      0.36      0.40        11\n",
      "          52       0.64      0.75      0.69        12\n",
      "          53       0.00      0.00      0.00         6\n",
      "          54       1.00      0.33      0.50         9\n",
      "          55       0.00      0.00      0.00         0\n",
      "          56       0.60      0.43      0.50         7\n",
      "          57       0.40      0.29      0.33        14\n",
      "          58       0.83      0.17      0.29        29\n",
      "          59       1.00      0.06      0.11        18\n",
      "          60       0.60      0.23      0.33        13\n",
      "          61       1.00      0.20      0.33         5\n",
      "          62       0.75      0.27      0.40        11\n",
      "          63       0.00      0.00      0.00         6\n",
      "          64       1.00      0.29      0.45        17\n",
      "          65       0.60      0.27      0.37        11\n",
      "          66       0.00      0.00      0.00        14\n",
      "          67       0.71      0.56      0.63         9\n",
      "          68       0.80      0.29      0.42        14\n",
      "          69       0.80      0.22      0.35        18\n",
      "          70       1.00      0.08      0.14        13\n",
      "          71       0.50      0.07      0.12        15\n",
      "          72       0.67      0.18      0.29        11\n",
      "          73       1.00      0.11      0.20         9\n",
      "          74       0.29      0.22      0.25         9\n",
      "          75       0.00      0.00      0.00         2\n",
      "          76       1.00      0.04      0.08        23\n",
      "          77       0.75      0.43      0.55         7\n",
      "          78       0.00      0.00      0.00         2\n",
      "          79       0.67      0.29      0.40         7\n",
      "          80       0.75      0.75      0.75         8\n",
      "          81       1.00      0.50      0.67        10\n",
      "          82       0.78      0.58      0.67        12\n",
      "          83       0.50      0.17      0.25         6\n",
      "          84       0.50      0.50      0.50         2\n",
      "          85       0.00      0.00      0.00         9\n",
      "          86       0.60      0.50      0.55         6\n",
      "          87       0.00      0.00      0.00         4\n",
      "          88       0.00      0.00      0.00         2\n",
      "          89       0.80      1.00      0.89         8\n",
      "          90       0.50      0.25      0.33         8\n",
      "          91       0.50      0.20      0.29         5\n",
      "          92       0.00      0.00      0.00         9\n",
      "          93       0.88      0.58      0.70        12\n",
      "          94       1.00      0.50      0.67         2\n",
      "          95       0.00      0.00      0.00         7\n",
      "          96       0.00      0.00      0.00        13\n",
      "          97       0.25      0.06      0.09        18\n",
      "          98       0.00      0.00      0.00         0\n",
      "          99       0.25      0.11      0.15         9\n",
      "         100       0.00      0.00      0.00         6\n",
      "         101       0.33      0.10      0.15        10\n",
      "         102       0.00      0.00      0.00        12\n",
      "         103       0.33      0.25      0.29         4\n",
      "         104       1.00      0.30      0.46        10\n",
      "         105       0.00      0.00      0.00         2\n",
      "         106       1.00      0.38      0.55         8\n",
      "         107       0.00      0.00      0.00        10\n",
      "         108       0.50      0.20      0.29        10\n",
      "         109       0.00      0.00      0.00         2\n",
      "         110       1.00      0.43      0.60         7\n",
      "         111       0.50      0.06      0.11        16\n",
      "         112       0.50      0.17      0.25         6\n",
      "         113       1.00      0.62      0.77         8\n",
      "         114       0.00      0.00      0.00         4\n",
      "         115       0.33      0.50      0.40         2\n",
      "         116       0.88      0.64      0.74        11\n",
      "         117       1.00      1.00      1.00         5\n",
      "         118       1.00      0.14      0.25         7\n",
      "         119       1.00      0.53      0.70        15\n",
      "         120       1.00      0.50      0.67         4\n",
      "         121       0.80      0.29      0.42        14\n",
      "         122       0.00      0.00      0.00         4\n",
      "         123       1.00      0.40      0.57         5\n",
      "         124       0.00      0.00      0.00         2\n",
      "         125       0.00      0.00      0.00         7\n",
      "         126       0.50      0.33      0.40         3\n",
      "         127       0.00      0.00      0.00         7\n",
      "         128       1.00      0.83      0.91         6\n",
      "         129       0.00      0.00      0.00         3\n",
      "         130       0.00      0.00      0.00         3\n",
      "         131       0.50      0.10      0.17        10\n",
      "         132       1.00      0.38      0.55         8\n",
      "         133       0.00      0.00      0.00         2\n",
      "         134       0.00      0.00      0.00         2\n",
      "         135       0.00      0.00      0.00         8\n",
      "         136       0.00      0.00      0.00         2\n",
      "         137       0.00      0.00      0.00         5\n",
      "         138       0.00      0.00      0.00         4\n",
      "         139       0.67      0.50      0.57         4\n",
      "         140       0.00      0.00      0.00         3\n",
      "         141       0.00      0.00      0.00         3\n",
      "         142       0.00      0.00      0.00         3\n",
      "         143       0.00      0.00      0.00         1\n",
      "         144       0.25      0.25      0.25         4\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       0.00      0.00      0.00         2\n",
      "         147       0.00      0.00      0.00         4\n",
      "         148       0.00      0.00      0.00         6\n",
      "         149       0.67      0.50      0.57         4\n",
      "         150       0.00      0.00      0.00         3\n",
      "         151       0.00      0.00      0.00         2\n",
      "         152       1.00      0.14      0.25         7\n",
      "         153       0.00      0.00      0.00         7\n",
      "         154       0.00      0.00      0.00         4\n",
      "         155       1.00      0.33      0.50         6\n",
      "         156       0.33      0.17      0.22         6\n",
      "         157       1.00      0.71      0.83         7\n",
      "         158       1.00      0.50      0.67         4\n",
      "         159       0.00      0.00      0.00         2\n",
      "         160       0.00      0.00      0.00         1\n",
      "         161       0.00      0.00      0.00         9\n",
      "         162       1.00      0.25      0.40         4\n",
      "         163       0.33      1.00      0.50         1\n",
      "         164       0.50      0.25      0.33         4\n",
      "         165       1.00      0.14      0.25         7\n",
      "         166       0.00      0.00      0.00         2\n",
      "         167       1.00      0.17      0.29         6\n",
      "         168       0.00      0.00      0.00         1\n",
      "         169       1.00      0.33      0.50         6\n",
      "         170       1.00      0.50      0.67         4\n",
      "         171       0.50      0.25      0.33         4\n",
      "         172       1.00      0.43      0.60         7\n",
      "         173       1.00      0.50      0.67         2\n",
      "         174       0.00      0.00      0.00         6\n",
      "         175       0.00      0.00      0.00         9\n",
      "         176       0.67      0.67      0.67         3\n",
      "         177       1.00      1.00      1.00         2\n",
      "         178       0.00      0.00      0.00         2\n",
      "         179       0.50      0.17      0.25         6\n",
      "         180       1.00      0.50      0.67         4\n",
      "         181       0.80      0.40      0.53        10\n",
      "         182       0.00      0.00      0.00        15\n",
      "         183       0.00      0.00      0.00         2\n",
      "         184       0.50      0.40      0.44         5\n",
      "         185       0.00      0.00      0.00         5\n",
      "         186       0.00      0.00      0.00         2\n",
      "         187       0.50      0.33      0.40         3\n",
      "         188       1.00      0.25      0.40         8\n",
      "         189       0.00      0.00      0.00         5\n",
      "         190       1.00      0.20      0.33         5\n",
      "         191       1.00      0.56      0.71         9\n",
      "         192       0.50      0.33      0.40         3\n",
      "         193       0.50      1.00      0.67         1\n",
      "         194       0.00      0.00      0.00         1\n",
      "         195       0.00      0.00      0.00         1\n",
      "         196       0.00      0.00      0.00         4\n",
      "         197       0.00      0.00      0.00         1\n",
      "         198       0.00      0.00      0.00         7\n",
      "         199       0.00      0.00      0.00         5\n",
      "         200       0.00      0.00      0.00         7\n",
      "         201       0.00      0.00      0.00         2\n",
      "         202       0.00      0.00      0.00         4\n",
      "         203       0.00      0.00      0.00         5\n",
      "         204       0.00      0.00      0.00         8\n",
      "         205       0.00      0.00      0.00         2\n",
      "         206       0.50      0.33      0.40         3\n",
      "         207       0.00      0.00      0.00         2\n",
      "         208       0.00      0.00      0.00         2\n",
      "         209       0.00      0.00      0.00         6\n",
      "         210       0.00      0.00      0.00         5\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       0.00      0.00      0.00         2\n",
      "         213       1.00      0.50      0.67         4\n",
      "         214       0.00      0.00      0.00         5\n",
      "         215       0.00      0.00      0.00         3\n",
      "         216       0.00      0.00      0.00         5\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.75      0.60      0.67         5\n",
      "         219       0.00      0.00      0.00         3\n",
      "         220       0.00      0.00      0.00         5\n",
      "         221       0.00      0.00      0.00         7\n",
      "         222       0.00      0.00      0.00         5\n",
      "         223       1.00      0.50      0.67         2\n",
      "         224       0.00      0.00      0.00         4\n",
      "         225       0.00      0.00      0.00         3\n",
      "         226       0.67      0.50      0.57         4\n",
      "         227       0.00      0.00      0.00         1\n",
      "         228       1.00      0.25      0.40         4\n",
      "         229       0.00      0.00      0.00         1\n",
      "         230       0.00      0.00      0.00         9\n",
      "         231       0.43      0.33      0.38         9\n",
      "         232       1.00      0.50      0.67         8\n",
      "         233       1.00      1.00      1.00         1\n",
      "         234       0.33      0.33      0.33         3\n",
      "         235       0.00      0.00      0.00         4\n",
      "         236       1.00      0.25      0.40         4\n",
      "         237       0.00      0.00      0.00         2\n",
      "         238       0.86      0.86      0.86         7\n",
      "         239       0.00      0.00      0.00         0\n",
      "         240       0.50      0.33      0.40         6\n",
      "         241       1.00      1.00      1.00         1\n",
      "         242       0.00      0.00      0.00         0\n",
      "         243       1.00      1.00      1.00         1\n",
      "         244       1.00      0.50      0.67         2\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         3\n",
      "         247       0.00      0.00      0.00         3\n",
      "         248       1.00      0.33      0.50         3\n",
      "         249       0.00      0.00      0.00         0\n",
      "         250       0.00      0.00      0.00         6\n",
      "         251       1.00      0.40      0.57         5\n",
      "         252       0.14      0.20      0.17         5\n",
      "         253       0.00      0.00      0.00         3\n",
      "         254       0.00      0.00      0.00         3\n",
      "         255       0.00      0.00      0.00         5\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.00      0.00      0.00         3\n",
      "         258       0.00      0.00      0.00         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       0.00      0.00      0.00         5\n",
      "         261       0.00      0.00      0.00         0\n",
      "         262       0.00      0.00      0.00         3\n",
      "         263       0.00      0.00      0.00         5\n",
      "         264       0.00      0.00      0.00         5\n",
      "         265       0.00      0.00      0.00         2\n",
      "         266       0.00      0.00      0.00         2\n",
      "         267       0.50      1.00      0.67         1\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       1.00      0.50      0.67         2\n",
      "         270       1.00      0.33      0.50         3\n",
      "         271       0.00      0.00      0.00         1\n",
      "         272       0.50      0.25      0.33         4\n",
      "         273       1.00      0.10      0.18        10\n",
      "         274       0.00      0.00      0.00         3\n",
      "         275       1.00      0.25      0.40         4\n",
      "         276       0.00      0.00      0.00         0\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         3\n",
      "         279       0.00      0.00      0.00         7\n",
      "         280       0.00      0.00      0.00         2\n",
      "         281       0.00      0.00      0.00         1\n",
      "         282       0.50      0.50      0.50         2\n",
      "         283       0.00      0.00      0.00         1\n",
      "         284       0.00      0.00      0.00         2\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         1\n",
      "         287       0.00      0.00      0.00         4\n",
      "         288       0.00      0.00      0.00         0\n",
      "         289       0.00      0.00      0.00         6\n",
      "         290       0.00      0.00      0.00         1\n",
      "         291       1.00      0.43      0.60         7\n",
      "         292       0.00      0.00      0.00         3\n",
      "         293       0.00      0.00      0.00         2\n",
      "         294       0.00      0.00      0.00         0\n",
      "         295       0.00      0.00      0.00         1\n",
      "         296       0.00      0.00      0.00         3\n",
      "         297       0.00      0.00      0.00         4\n",
      "         298       0.00      0.00      0.00         2\n",
      "         299       0.00      0.00      0.00         0\n",
      "         300       1.00      1.00      1.00         1\n",
      "         301       0.00      0.00      0.00         4\n",
      "         302       0.00      0.00      0.00         0\n",
      "         303       0.00      0.00      0.00         0\n",
      "         304       0.33      0.17      0.22         6\n",
      "         305       0.00      0.00      0.00         3\n",
      "         306       0.00      0.00      0.00         0\n",
      "         307       0.00      0.00      0.00         4\n",
      "         308       1.00      0.50      0.67         2\n",
      "         309       0.00      0.00      0.00         3\n",
      "         310       0.00      0.00      0.00         1\n",
      "         311       0.00      0.00      0.00         2\n",
      "         312       1.00      0.50      0.67         2\n",
      "         313       0.00      0.00      0.00         4\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         0\n",
      "         316       1.00      0.57      0.73         7\n",
      "         317       0.00      0.00      0.00         1\n",
      "         318       0.50      0.50      0.50         2\n",
      "         319       0.00      0.00      0.00         3\n",
      "         320       1.00      0.33      0.50         6\n",
      "         321       0.00      0.00      0.00         2\n",
      "         322       1.00      1.00      1.00         1\n",
      "         323       0.00      0.00      0.00         4\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       0.00      0.00      0.00         2\n",
      "         327       0.00      0.00      0.00         2\n",
      "         328       0.50      0.50      0.50         2\n",
      "         329       0.00      0.00      0.00         5\n",
      "         330       1.00      0.33      0.50         3\n",
      "         331       0.00      0.00      0.00         3\n",
      "         332       0.00      0.00      0.00         2\n",
      "         333       0.00      0.00      0.00         1\n",
      "         334       0.67      0.50      0.57         4\n",
      "         335       1.00      0.33      0.50         3\n",
      "         336       0.00      0.00      0.00         1\n",
      "         337       1.00      0.50      0.67         2\n",
      "         338       0.00      0.00      0.00         2\n",
      "         339       0.00      0.00      0.00         1\n",
      "         340       0.00      0.00      0.00         0\n",
      "         341       0.00      0.00      0.00         1\n",
      "         342       0.00      0.00      0.00         3\n",
      "         343       0.00      0.00      0.00         9\n",
      "         344       0.00      0.00      0.00         1\n",
      "         345       1.00      1.00      1.00         1\n",
      "         346       0.00      0.00      0.00         1\n",
      "         347       0.00      0.00      0.00         0\n",
      "         348       1.00      1.00      1.00         1\n",
      "         349       1.00      0.33      0.50         3\n",
      "         350       1.00      0.25      0.40         4\n",
      "         351       0.00      0.00      0.00         4\n",
      "         352       0.00      0.00      0.00         6\n",
      "         353       0.00      0.00      0.00         0\n",
      "         354       0.00      0.00      0.00         6\n",
      "         355       0.00      0.00      0.00         5\n",
      "         356       0.00      0.00      0.00         2\n",
      "         357       0.75      0.75      0.75         4\n",
      "         358       0.00      0.00      0.00         0\n",
      "         359       0.00      0.00      0.00         1\n",
      "         360       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         0\n",
      "         362       1.00      1.00      1.00         1\n",
      "         363       0.00      0.00      0.00         1\n",
      "         364       0.00      0.00      0.00         1\n",
      "         365       1.00      1.00      1.00         1\n",
      "         366       0.00      0.00      0.00         4\n",
      "         367       1.00      0.33      0.50         3\n",
      "         368       0.00      0.00      0.00         2\n",
      "         369       0.00      0.00      0.00         4\n",
      "         370       0.00      0.00      0.00         2\n",
      "         371       1.00      0.50      0.67         6\n",
      "         372       0.00      0.00      0.00         4\n",
      "         373       0.00      0.00      0.00         0\n",
      "         374       1.00      0.50      0.67         4\n",
      "         375       0.00      0.00      0.00         3\n",
      "         376       0.00      0.00      0.00         1\n",
      "         377       0.00      0.00      0.00         4\n",
      "         378       1.00      0.50      0.67         4\n",
      "         379       0.00      0.00      0.00         4\n",
      "         380       0.00      0.00      0.00         3\n",
      "         381       0.00      0.00      0.00         1\n",
      "         382       0.00      0.00      0.00         2\n",
      "         383       1.00      1.00      1.00         2\n",
      "         384       1.00      1.00      1.00         1\n",
      "         385       0.00      0.00      0.00         0\n",
      "         386       0.00      0.00      0.00         3\n",
      "         387       0.00      0.00      0.00         0\n",
      "         388       0.00      0.00      0.00         3\n",
      "         389       0.00      0.00      0.00         2\n",
      "         390       0.00      0.00      0.00         2\n",
      "         391       0.00      0.00      0.00         3\n",
      "         392       0.00      0.00      0.00         5\n",
      "         393       0.00      0.00      0.00         1\n",
      "         394       0.00      0.00      0.00         3\n",
      "         395       0.00      0.00      0.00         1\n",
      "         396       0.00      0.00      0.00         1\n",
      "         397       0.00      0.00      0.00         4\n",
      "         398       0.00      0.00      0.00         3\n",
      "         399       0.00      0.00      0.00         2\n",
      "         400       0.50      0.33      0.40         3\n",
      "         401       0.00      0.00      0.00         1\n",
      "         402       0.00      0.00      0.00         1\n",
      "         403       0.00      0.00      0.00         1\n",
      "         404       0.00      0.00      0.00         0\n",
      "         405       0.00      0.00      0.00         1\n",
      "         406       0.00      0.00      0.00         3\n",
      "         407       0.00      0.00      0.00         2\n",
      "         408       0.00      0.00      0.00         2\n",
      "         409       0.00      0.00      0.00         0\n",
      "         410       0.00      0.00      0.00         0\n",
      "         411       0.00      0.00      0.00         6\n",
      "         412       0.00      0.00      0.00         5\n",
      "         413       0.00      0.00      0.00         3\n",
      "         414       0.00      0.00      0.00         0\n",
      "         415       1.00      0.50      0.67         2\n",
      "         416       0.00      0.00      0.00         0\n",
      "         417       0.00      0.00      0.00         1\n",
      "         418       0.00      0.00      0.00         0\n",
      "         419       0.00      0.00      0.00         3\n",
      "         420       0.00      0.00      0.00         2\n",
      "         421       0.00      0.00      0.00         2\n",
      "         422       0.00      0.00      0.00         1\n",
      "         423       0.00      0.00      0.00         0\n",
      "         424       0.00      0.00      0.00         1\n",
      "         425       0.00      0.00      0.00         0\n",
      "         426       1.00      1.00      1.00         1\n",
      "         427       0.00      0.00      0.00         2\n",
      "         428       0.00      0.00      0.00         0\n",
      "         429       0.00      0.00      0.00         0\n",
      "         430       0.00      0.00      0.00         2\n",
      "         431       0.00      0.00      0.00         0\n",
      "         432       0.00      0.00      0.00         0\n",
      "         433       0.00      0.00      0.00         1\n",
      "         434       0.00      0.00      0.00         1\n",
      "         435       0.00      0.00      0.00         3\n",
      "         436       0.50      1.00      0.67         2\n",
      "         437       0.00      0.00      0.00         1\n",
      "         438       1.00      0.50      0.67         2\n",
      "         439       0.00      0.00      0.00         3\n",
      "         440       0.00      0.00      0.00         2\n",
      "         441       0.00      0.00      0.00         1\n",
      "         442       1.00      1.00      1.00         1\n",
      "         443       1.00      1.00      1.00         1\n",
      "         444       0.00      0.00      0.00         1\n",
      "         445       0.00      0.00      0.00         4\n",
      "         446       0.00      0.00      0.00         5\n",
      "         447       0.00      0.00      0.00         3\n",
      "         448       0.00      0.00      0.00         0\n",
      "         449       0.00      0.00      0.00         0\n",
      "         450       0.00      0.00      0.00         0\n",
      "         451       0.00      0.00      0.00         2\n",
      "         452       0.00      0.00      0.00         4\n",
      "         453       0.00      0.00      0.00         0\n",
      "         454       0.00      0.00      0.00         0\n",
      "         455       1.00      0.50      0.67         4\n",
      "         456       0.00      0.00      0.00         0\n",
      "         457       0.00      0.00      0.00         0\n",
      "         458       0.00      0.00      0.00         3\n",
      "         459       1.00      1.00      1.00         1\n",
      "         460       0.00      0.00      0.00         2\n",
      "         461       1.00      1.00      1.00         1\n",
      "         462       0.00      0.00      0.00         2\n",
      "         463       0.00      0.00      0.00         0\n",
      "         464       0.00      0.00      0.00         0\n",
      "         465       0.00      0.00      0.00         2\n",
      "         466       0.00      0.00      0.00         3\n",
      "         467       0.00      0.00      0.00         2\n",
      "         468       0.00      0.00      0.00         1\n",
      "         469       0.00      0.00      0.00         0\n",
      "         470       1.00      0.50      0.67         2\n",
      "         471       0.00      0.00      0.00         2\n",
      "         472       0.00      0.00      0.00         3\n",
      "         473       0.00      0.00      0.00         1\n",
      "         474       0.00      0.00      0.00         0\n",
      "         475       0.00      0.00      0.00         0\n",
      "         476       0.00      0.00      0.00         3\n",
      "         477       1.00      0.50      0.67         6\n",
      "         478       0.00      0.00      0.00         3\n",
      "         479       0.00      0.00      0.00         2\n",
      "         480       1.00      1.00      1.00         1\n",
      "         481       0.00      0.00      0.00         3\n",
      "         482       0.00      0.00      0.00         1\n",
      "         483       0.00      0.00      0.00         2\n",
      "         484       0.00      0.00      0.00         2\n",
      "         485       1.00      0.25      0.40         4\n",
      "         486       0.00      0.00      0.00         0\n",
      "         487       0.00      0.00      0.00         3\n",
      "         488       1.00      0.33      0.50         3\n",
      "         489       0.00      0.00      0.00         0\n",
      "         490       0.00      0.00      0.00         2\n",
      "         491       0.00      0.00      0.00         0\n",
      "         492       0.00      0.00      0.00         0\n",
      "         493       0.00      0.00      0.00         2\n",
      "         494       0.00      0.00      0.00         3\n",
      "         495       0.00      0.00      0.00         5\n",
      "         496       0.00      0.00      0.00         2\n",
      "         497       0.00      0.00      0.00         3\n",
      "         498       0.33      1.00      0.50         1\n",
      "         499       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.65      0.29      0.40      3662\n",
      "   macro avg       0.31      0.18      0.21      3662\n",
      "weighted avg       0.54      0.29      0.36      3662\n",
      " samples avg       0.39      0.29      0.31      3662\n",
      "\n",
      "Time taken to run this cell : 0:00:05.363675\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.001, penalty='l1'), n_jobs=-2)\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict (x_test_multilabel)\n",
    "\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+--------------------+----------+\n",
      "|      Model name     |  Parameter   |      f1_score      | Accuracy |\n",
      "+---------------------+--------------+--------------------+----------+\n",
      "| Logistic Regression | alpha = 1e-3 | 0.4029400678477196 |  0.178   |\n",
      "+---------------------+--------------+--------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Model name\", \"Parameter\", \"f1_score\", \"Accuracy\"]\n",
    "x.add_row([\"Logistic Regression\",\"alpha = 1e-3\", f1, str(metrics.accuracy_score(y_test, predictions))])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=OneVsRestClassifier(estimator=SGDClassifier(loss='log'),\n",
       "                                           n_jobs=-2),\n",
       "             param_grid={'estimator__alpha': [1e-05, 0.001, 0.1, 10],\n",
       "                         'estimator__penalty': ['l1']},\n",
       "             scoring='f1_micro')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/12632992/gridsearch-for-an-estimator-inside-a-onevsrestclassifier\n",
    "'''When you use nested estimators with grid search you can scope the parameters with __ as a separator. \n",
    "In this case the SVC model is stored as an attribute named estimator inside the OneVsRestClassifier model:'''\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import *\n",
    "\n",
    "mlclf = OneVsRestClassifier(SGDClassifier(loss='log'), n_jobs=-2)\n",
    "parameters={'estimator__alpha':[10**i for i in range(-5,3,2)]\n",
    "            ,'estimator__penalty':['l1']}\n",
    "\n",
    "grid = GridSearchCV(mlclf, parameters,cv=3,scoring='f1_micro')\n",
    "grid.fit(x_train_multilabel, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f1_micro'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.181\n",
      "Hamming loss  0.003161\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3011, Recall: 0.1731, F1-measure: 0.2043\n",
      "Micro-average quality numbers\n",
      "Precision: 0.6542, Recall: 0.2903, F1-measure: 0.4021\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.28      0.35        97\n",
      "           1       0.74      0.48      0.58       140\n",
      "           2       0.63      0.35      0.45       125\n",
      "           3       0.82      0.60      0.70        91\n",
      "           4       0.76      0.50      0.60        88\n",
      "           5       0.77      0.44      0.56       136\n",
      "           6       0.27      0.12      0.17        25\n",
      "           7       0.82      0.33      0.47        27\n",
      "           8       0.88      0.52      0.66        88\n",
      "           9       0.83      0.42      0.56        81\n",
      "          10       0.76      0.43      0.55        61\n",
      "          11       0.74      0.71      0.72        82\n",
      "          12       0.44      0.20      0.28        40\n",
      "          13       0.11      0.03      0.05        34\n",
      "          14       0.55      0.12      0.20        89\n",
      "          15       0.44      0.26      0.33        46\n",
      "          16       0.68      0.49      0.57        39\n",
      "          17       0.33      0.23      0.27        22\n",
      "          18       0.71      0.21      0.32        57\n",
      "          19       0.44      0.18      0.25        45\n",
      "          20       0.40      0.29      0.33        21\n",
      "          21       0.67      0.64      0.65        56\n",
      "          22       0.67      0.44      0.53        18\n",
      "          23       0.69      0.30      0.42        30\n",
      "          24       0.67      0.36      0.47        11\n",
      "          25       0.44      0.14      0.21        29\n",
      "          26       0.25      0.11      0.15         9\n",
      "          27       0.56      0.23      0.33        39\n",
      "          28       0.50      0.44      0.47         9\n",
      "          29       0.75      0.75      0.75        16\n",
      "          30       0.88      0.30      0.45        23\n",
      "          31       0.67      0.21      0.32        29\n",
      "          32       0.83      0.48      0.61        21\n",
      "          33       0.00      0.00      0.00         6\n",
      "          34       0.88      0.78      0.82        45\n",
      "          35       0.25      0.07      0.11        29\n",
      "          36       0.80      0.40      0.53        10\n",
      "          37       0.50      0.12      0.20         8\n",
      "          38       0.60      0.38      0.46         8\n",
      "          39       0.00      0.00      0.00        17\n",
      "          40       1.00      0.09      0.17        11\n",
      "          41       0.50      0.22      0.31         9\n",
      "          42       0.33      0.14      0.20         7\n",
      "          43       0.75      0.75      0.75         4\n",
      "          44       1.00      0.43      0.60         7\n",
      "          45       0.33      0.14      0.20         7\n",
      "          46       0.69      0.32      0.44        28\n",
      "          47       0.00      0.00      0.00         2\n",
      "          48       0.50      0.14      0.22         7\n",
      "          49       0.40      0.29      0.33         7\n",
      "          50       1.00      0.50      0.67        10\n",
      "          51       0.44      0.36      0.40        11\n",
      "          52       0.64      0.75      0.69        12\n",
      "          53       0.00      0.00      0.00         6\n",
      "          54       1.00      0.33      0.50         9\n",
      "          55       0.00      0.00      0.00         0\n",
      "          56       0.60      0.43      0.50         7\n",
      "          57       0.44      0.29      0.35        14\n",
      "          58       0.80      0.14      0.24        29\n",
      "          59       1.00      0.11      0.20        18\n",
      "          60       0.60      0.23      0.33        13\n",
      "          61       1.00      0.20      0.33         5\n",
      "          62       0.75      0.27      0.40        11\n",
      "          63       0.00      0.00      0.00         6\n",
      "          64       1.00      0.29      0.45        17\n",
      "          65       0.60      0.27      0.37        11\n",
      "          66       0.00      0.00      0.00        14\n",
      "          67       0.71      0.56      0.63         9\n",
      "          68       0.80      0.29      0.42        14\n",
      "          69       0.80      0.22      0.35        18\n",
      "          70       1.00      0.08      0.14        13\n",
      "          71       0.50      0.07      0.12        15\n",
      "          72       0.75      0.27      0.40        11\n",
      "          73       1.00      0.11      0.20         9\n",
      "          74       0.33      0.22      0.27         9\n",
      "          75       0.00      0.00      0.00         2\n",
      "          76       1.00      0.04      0.08        23\n",
      "          77       0.75      0.43      0.55         7\n",
      "          78       0.00      0.00      0.00         2\n",
      "          79       0.67      0.29      0.40         7\n",
      "          80       1.00      0.62      0.77         8\n",
      "          81       1.00      0.50      0.67        10\n",
      "          82       0.78      0.58      0.67        12\n",
      "          83       0.50      0.17      0.25         6\n",
      "          84       0.50      0.50      0.50         2\n",
      "          85       0.00      0.00      0.00         9\n",
      "          86       0.60      0.50      0.55         6\n",
      "          87       0.00      0.00      0.00         4\n",
      "          88       0.00      0.00      0.00         2\n",
      "          89       0.80      1.00      0.89         8\n",
      "          90       0.50      0.25      0.33         8\n",
      "          91       0.50      0.20      0.29         5\n",
      "          92       0.00      0.00      0.00         9\n",
      "          93       0.88      0.58      0.70        12\n",
      "          94       1.00      0.50      0.67         2\n",
      "          95       0.00      0.00      0.00         7\n",
      "          96       0.00      0.00      0.00        13\n",
      "          97       0.25      0.06      0.09        18\n",
      "          98       0.00      0.00      0.00         0\n",
      "          99       0.25      0.11      0.15         9\n",
      "         100       0.00      0.00      0.00         6\n",
      "         101       0.33      0.10      0.15        10\n",
      "         102       0.00      0.00      0.00        12\n",
      "         103       0.50      0.50      0.50         4\n",
      "         104       1.00      0.30      0.46        10\n",
      "         105       0.00      0.00      0.00         2\n",
      "         106       1.00      0.25      0.40         8\n",
      "         107       0.00      0.00      0.00        10\n",
      "         108       0.50      0.10      0.17        10\n",
      "         109       0.00      0.00      0.00         2\n",
      "         110       1.00      0.43      0.60         7\n",
      "         111       0.67      0.12      0.21        16\n",
      "         112       0.50      0.17      0.25         6\n",
      "         113       1.00      0.50      0.67         8\n",
      "         114       0.00      0.00      0.00         4\n",
      "         115       1.00      0.50      0.67         2\n",
      "         116       0.88      0.64      0.74        11\n",
      "         117       1.00      1.00      1.00         5\n",
      "         118       1.00      0.14      0.25         7\n",
      "         119       1.00      0.53      0.70        15\n",
      "         120       1.00      0.50      0.67         4\n",
      "         121       0.80      0.29      0.42        14\n",
      "         122       0.00      0.00      0.00         4\n",
      "         123       1.00      0.40      0.57         5\n",
      "         124       0.00      0.00      0.00         2\n",
      "         125       0.00      0.00      0.00         7\n",
      "         126       0.50      0.33      0.40         3\n",
      "         127       0.00      0.00      0.00         7\n",
      "         128       1.00      0.83      0.91         6\n",
      "         129       0.00      0.00      0.00         3\n",
      "         130       0.00      0.00      0.00         3\n",
      "         131       0.50      0.10      0.17        10\n",
      "         132       1.00      0.25      0.40         8\n",
      "         133       0.00      0.00      0.00         2\n",
      "         134       0.00      0.00      0.00         2\n",
      "         135       0.00      0.00      0.00         8\n",
      "         136       0.00      0.00      0.00         2\n",
      "         137       0.00      0.00      0.00         5\n",
      "         138       0.00      0.00      0.00         4\n",
      "         139       0.67      0.50      0.57         4\n",
      "         140       0.00      0.00      0.00         3\n",
      "         141       0.00      0.00      0.00         3\n",
      "         142       0.00      0.00      0.00         3\n",
      "         143       0.00      0.00      0.00         1\n",
      "         144       0.25      0.25      0.25         4\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       0.00      0.00      0.00         2\n",
      "         147       0.00      0.00      0.00         4\n",
      "         148       0.50      0.17      0.25         6\n",
      "         149       0.67      0.50      0.57         4\n",
      "         150       0.00      0.00      0.00         3\n",
      "         151       0.00      0.00      0.00         2\n",
      "         152       1.00      0.14      0.25         7\n",
      "         153       0.00      0.00      0.00         7\n",
      "         154       0.00      0.00      0.00         4\n",
      "         155       1.00      0.33      0.50         6\n",
      "         156       0.33      0.17      0.22         6\n",
      "         157       1.00      0.71      0.83         7\n",
      "         158       1.00      0.50      0.67         4\n",
      "         159       0.00      0.00      0.00         2\n",
      "         160       0.00      0.00      0.00         1\n",
      "         161       0.00      0.00      0.00         9\n",
      "         162       1.00      0.25      0.40         4\n",
      "         163       0.33      1.00      0.50         1\n",
      "         164       0.50      0.25      0.33         4\n",
      "         165       1.00      0.14      0.25         7\n",
      "         166       0.00      0.00      0.00         2\n",
      "         167       1.00      0.33      0.50         6\n",
      "         168       0.00      0.00      0.00         1\n",
      "         169       1.00      0.33      0.50         6\n",
      "         170       1.00      0.50      0.67         4\n",
      "         171       0.50      0.25      0.33         4\n",
      "         172       1.00      0.43      0.60         7\n",
      "         173       1.00      0.50      0.67         2\n",
      "         174       0.00      0.00      0.00         6\n",
      "         175       0.00      0.00      0.00         9\n",
      "         176       0.67      0.67      0.67         3\n",
      "         177       1.00      1.00      1.00         2\n",
      "         178       0.00      0.00      0.00         2\n",
      "         179       0.50      0.17      0.25         6\n",
      "         180       1.00      0.50      0.67         4\n",
      "         181       0.80      0.40      0.53        10\n",
      "         182       0.00      0.00      0.00        15\n",
      "         183       0.00      0.00      0.00         2\n",
      "         184       0.50      0.40      0.44         5\n",
      "         185       0.00      0.00      0.00         5\n",
      "         186       0.00      0.00      0.00         2\n",
      "         187       0.50      0.33      0.40         3\n",
      "         188       1.00      0.25      0.40         8\n",
      "         189       0.00      0.00      0.00         5\n",
      "         190       1.00      0.20      0.33         5\n",
      "         191       1.00      0.56      0.71         9\n",
      "         192       0.50      0.33      0.40         3\n",
      "         193       0.50      1.00      0.67         1\n",
      "         194       0.00      0.00      0.00         1\n",
      "         195       0.00      0.00      0.00         1\n",
      "         196       0.00      0.00      0.00         4\n",
      "         197       0.00      0.00      0.00         1\n",
      "         198       0.00      0.00      0.00         7\n",
      "         199       0.00      0.00      0.00         5\n",
      "         200       0.00      0.00      0.00         7\n",
      "         201       0.00      0.00      0.00         2\n",
      "         202       0.00      0.00      0.00         4\n",
      "         203       0.00      0.00      0.00         5\n",
      "         204       0.00      0.00      0.00         8\n",
      "         205       0.00      0.00      0.00         2\n",
      "         206       0.50      0.33      0.40         3\n",
      "         207       0.00      0.00      0.00         2\n",
      "         208       0.00      0.00      0.00         2\n",
      "         209       0.00      0.00      0.00         6\n",
      "         210       0.00      0.00      0.00         5\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       0.00      0.00      0.00         2\n",
      "         213       1.00      0.50      0.67         4\n",
      "         214       0.00      0.00      0.00         5\n",
      "         215       0.00      0.00      0.00         3\n",
      "         216       0.00      0.00      0.00         5\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.75      0.60      0.67         5\n",
      "         219       0.00      0.00      0.00         3\n",
      "         220       0.00      0.00      0.00         5\n",
      "         221       0.00      0.00      0.00         7\n",
      "         222       0.00      0.00      0.00         5\n",
      "         223       0.00      0.00      0.00         2\n",
      "         224       0.00      0.00      0.00         4\n",
      "         225       0.00      0.00      0.00         3\n",
      "         226       0.67      0.50      0.57         4\n",
      "         227       0.00      0.00      0.00         1\n",
      "         228       1.00      0.25      0.40         4\n",
      "         229       0.00      0.00      0.00         1\n",
      "         230       0.00      0.00      0.00         9\n",
      "         231       0.43      0.33      0.38         9\n",
      "         232       1.00      0.50      0.67         8\n",
      "         233       1.00      1.00      1.00         1\n",
      "         234       0.33      0.33      0.33         3\n",
      "         235       0.00      0.00      0.00         4\n",
      "         236       1.00      0.25      0.40         4\n",
      "         237       0.00      0.00      0.00         2\n",
      "         238       0.83      0.71      0.77         7\n",
      "         239       0.00      0.00      0.00         0\n",
      "         240       0.50      0.33      0.40         6\n",
      "         241       1.00      1.00      1.00         1\n",
      "         242       0.00      0.00      0.00         0\n",
      "         243       0.00      0.00      0.00         1\n",
      "         244       1.00      0.50      0.67         2\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.00      0.00      0.00         3\n",
      "         247       0.00      0.00      0.00         3\n",
      "         248       1.00      0.33      0.50         3\n",
      "         249       0.00      0.00      0.00         0\n",
      "         250       0.00      0.00      0.00         6\n",
      "         251       1.00      0.40      0.57         5\n",
      "         252       0.14      0.20      0.17         5\n",
      "         253       0.00      0.00      0.00         3\n",
      "         254       0.00      0.00      0.00         3\n",
      "         255       0.00      0.00      0.00         5\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.00      0.00      0.00         3\n",
      "         258       1.00      0.25      0.40         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       0.00      0.00      0.00         5\n",
      "         261       0.00      0.00      0.00         0\n",
      "         262       0.00      0.00      0.00         3\n",
      "         263       0.00      0.00      0.00         5\n",
      "         264       0.00      0.00      0.00         5\n",
      "         265       0.00      0.00      0.00         2\n",
      "         266       0.00      0.00      0.00         2\n",
      "         267       0.50      1.00      0.67         1\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       0.00      0.00      0.00         2\n",
      "         270       1.00      0.33      0.50         3\n",
      "         271       0.00      0.00      0.00         1\n",
      "         272       0.50      0.25      0.33         4\n",
      "         273       0.00      0.00      0.00        10\n",
      "         274       0.00      0.00      0.00         3\n",
      "         275       1.00      0.25      0.40         4\n",
      "         276       0.00      0.00      0.00         0\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         3\n",
      "         279       0.00      0.00      0.00         7\n",
      "         280       0.00      0.00      0.00         2\n",
      "         281       0.00      0.00      0.00         1\n",
      "         282       0.50      0.50      0.50         2\n",
      "         283       0.00      0.00      0.00         1\n",
      "         284       0.00      0.00      0.00         2\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         1\n",
      "         287       0.00      0.00      0.00         4\n",
      "         288       0.00      0.00      0.00         0\n",
      "         289       0.00      0.00      0.00         6\n",
      "         290       0.00      0.00      0.00         1\n",
      "         291       1.00      0.43      0.60         7\n",
      "         292       0.00      0.00      0.00         3\n",
      "         293       0.00      0.00      0.00         2\n",
      "         294       0.00      0.00      0.00         0\n",
      "         295       0.00      0.00      0.00         1\n",
      "         296       0.00      0.00      0.00         3\n",
      "         297       0.00      0.00      0.00         4\n",
      "         298       0.00      0.00      0.00         2\n",
      "         299       0.00      0.00      0.00         0\n",
      "         300       1.00      1.00      1.00         1\n",
      "         301       0.00      0.00      0.00         4\n",
      "         302       0.00      0.00      0.00         0\n",
      "         303       0.00      0.00      0.00         0\n",
      "         304       0.25      0.17      0.20         6\n",
      "         305       0.00      0.00      0.00         3\n",
      "         306       0.00      0.00      0.00         0\n",
      "         307       0.00      0.00      0.00         4\n",
      "         308       1.00      0.50      0.67         2\n",
      "         309       0.00      0.00      0.00         3\n",
      "         310       0.00      0.00      0.00         1\n",
      "         311       0.00      0.00      0.00         2\n",
      "         312       1.00      0.50      0.67         2\n",
      "         313       0.00      0.00      0.00         4\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         0\n",
      "         316       1.00      0.29      0.44         7\n",
      "         317       0.00      0.00      0.00         1\n",
      "         318       0.33      0.50      0.40         2\n",
      "         319       0.00      0.00      0.00         3\n",
      "         320       1.00      0.50      0.67         6\n",
      "         321       0.00      0.00      0.00         2\n",
      "         322       1.00      1.00      1.00         1\n",
      "         323       0.00      0.00      0.00         4\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       0.00      0.00      0.00         2\n",
      "         327       0.00      0.00      0.00         2\n",
      "         328       0.50      0.50      0.50         2\n",
      "         329       0.00      0.00      0.00         5\n",
      "         330       1.00      0.33      0.50         3\n",
      "         331       0.00      0.00      0.00         3\n",
      "         332       0.00      0.00      0.00         2\n",
      "         333       0.00      0.00      0.00         1\n",
      "         334       0.67      0.50      0.57         4\n",
      "         335       1.00      0.33      0.50         3\n",
      "         336       0.00      0.00      0.00         1\n",
      "         337       1.00      0.50      0.67         2\n",
      "         338       0.00      0.00      0.00         2\n",
      "         339       0.00      0.00      0.00         1\n",
      "         340       0.00      0.00      0.00         0\n",
      "         341       0.00      0.00      0.00         1\n",
      "         342       0.00      0.00      0.00         3\n",
      "         343       0.00      0.00      0.00         9\n",
      "         344       0.00      0.00      0.00         1\n",
      "         345       1.00      1.00      1.00         1\n",
      "         346       0.00      0.00      0.00         1\n",
      "         347       0.00      0.00      0.00         0\n",
      "         348       1.00      1.00      1.00         1\n",
      "         349       1.00      0.33      0.50         3\n",
      "         350       0.00      0.00      0.00         4\n",
      "         351       0.00      0.00      0.00         4\n",
      "         352       0.00      0.00      0.00         6\n",
      "         353       0.00      0.00      0.00         0\n",
      "         354       0.00      0.00      0.00         6\n",
      "         355       0.00      0.00      0.00         5\n",
      "         356       0.00      0.00      0.00         2\n",
      "         357       0.75      0.75      0.75         4\n",
      "         358       0.00      0.00      0.00         0\n",
      "         359       0.00      0.00      0.00         1\n",
      "         360       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         0\n",
      "         362       1.00      1.00      1.00         1\n",
      "         363       0.00      0.00      0.00         1\n",
      "         364       0.00      0.00      0.00         1\n",
      "         365       1.00      1.00      1.00         1\n",
      "         366       0.00      0.00      0.00         4\n",
      "         367       1.00      0.33      0.50         3\n",
      "         368       0.00      0.00      0.00         2\n",
      "         369       0.00      0.00      0.00         4\n",
      "         370       0.00      0.00      0.00         2\n",
      "         371       1.00      0.50      0.67         6\n",
      "         372       0.00      0.00      0.00         4\n",
      "         373       0.00      0.00      0.00         0\n",
      "         374       1.00      0.50      0.67         4\n",
      "         375       0.00      0.00      0.00         3\n",
      "         376       0.00      0.00      0.00         1\n",
      "         377       0.00      0.00      0.00         4\n",
      "         378       1.00      0.50      0.67         4\n",
      "         379       0.00      0.00      0.00         4\n",
      "         380       0.00      0.00      0.00         3\n",
      "         381       0.00      0.00      0.00         1\n",
      "         382       0.00      0.00      0.00         2\n",
      "         383       0.67      1.00      0.80         2\n",
      "         384       1.00      1.00      1.00         1\n",
      "         385       0.00      0.00      0.00         0\n",
      "         386       0.00      0.00      0.00         3\n",
      "         387       0.00      0.00      0.00         0\n",
      "         388       0.00      0.00      0.00         3\n",
      "         389       0.00      0.00      0.00         2\n",
      "         390       0.00      0.00      0.00         2\n",
      "         391       0.00      0.00      0.00         3\n",
      "         392       0.00      0.00      0.00         5\n",
      "         393       0.00      0.00      0.00         1\n",
      "         394       0.00      0.00      0.00         3\n",
      "         395       0.00      0.00      0.00         1\n",
      "         396       0.00      0.00      0.00         1\n",
      "         397       0.00      0.00      0.00         4\n",
      "         398       0.00      0.00      0.00         3\n",
      "         399       0.00      0.00      0.00         2\n",
      "         400       0.50      0.33      0.40         3\n",
      "         401       0.00      0.00      0.00         1\n",
      "         402       0.00      0.00      0.00         1\n",
      "         403       0.00      0.00      0.00         1\n",
      "         404       0.00      0.00      0.00         0\n",
      "         405       0.00      0.00      0.00         1\n",
      "         406       0.00      0.00      0.00         3\n",
      "         407       0.00      0.00      0.00         2\n",
      "         408       1.00      0.50      0.67         2\n",
      "         409       0.00      0.00      0.00         0\n",
      "         410       0.00      0.00      0.00         0\n",
      "         411       0.00      0.00      0.00         6\n",
      "         412       0.00      0.00      0.00         5\n",
      "         413       0.00      0.00      0.00         3\n",
      "         414       0.00      0.00      0.00         0\n",
      "         415       0.00      0.00      0.00         2\n",
      "         416       0.00      0.00      0.00         0\n",
      "         417       0.00      0.00      0.00         1\n",
      "         418       0.00      0.00      0.00         0\n",
      "         419       0.00      0.00      0.00         3\n",
      "         420       0.00      0.00      0.00         2\n",
      "         421       0.00      0.00      0.00         2\n",
      "         422       0.00      0.00      0.00         1\n",
      "         423       0.00      0.00      0.00         0\n",
      "         424       0.00      0.00      0.00         1\n",
      "         425       0.00      0.00      0.00         0\n",
      "         426       1.00      1.00      1.00         1\n",
      "         427       0.00      0.00      0.00         2\n",
      "         428       0.00      0.00      0.00         0\n",
      "         429       0.00      0.00      0.00         0\n",
      "         430       0.00      0.00      0.00         2\n",
      "         431       0.00      0.00      0.00         0\n",
      "         432       0.00      0.00      0.00         0\n",
      "         433       0.00      0.00      0.00         1\n",
      "         434       0.00      0.00      0.00         1\n",
      "         435       0.00      0.00      0.00         3\n",
      "         436       0.50      1.00      0.67         2\n",
      "         437       0.00      0.00      0.00         1\n",
      "         438       1.00      0.50      0.67         2\n",
      "         439       0.00      0.00      0.00         3\n",
      "         440       0.00      0.00      0.00         2\n",
      "         441       0.00      0.00      0.00         1\n",
      "         442       0.00      0.00      0.00         1\n",
      "         443       1.00      1.00      1.00         1\n",
      "         444       0.00      0.00      0.00         1\n",
      "         445       0.00      0.00      0.00         4\n",
      "         446       0.00      0.00      0.00         5\n",
      "         447       0.00      0.00      0.00         3\n",
      "         448       0.00      0.00      0.00         0\n",
      "         449       0.00      0.00      0.00         0\n",
      "         450       0.00      0.00      0.00         0\n",
      "         451       0.00      0.00      0.00         2\n",
      "         452       0.00      0.00      0.00         4\n",
      "         453       0.00      0.00      0.00         0\n",
      "         454       0.00      0.00      0.00         0\n",
      "         455       1.00      0.50      0.67         4\n",
      "         456       0.00      0.00      0.00         0\n",
      "         457       0.00      0.00      0.00         0\n",
      "         458       0.00      0.00      0.00         3\n",
      "         459       1.00      1.00      1.00         1\n",
      "         460       0.00      0.00      0.00         2\n",
      "         461       1.00      1.00      1.00         1\n",
      "         462       0.00      0.00      0.00         2\n",
      "         463       0.00      0.00      0.00         0\n",
      "         464       0.00      0.00      0.00         0\n",
      "         465       0.00      0.00      0.00         2\n",
      "         466       0.00      0.00      0.00         3\n",
      "         467       0.00      0.00      0.00         2\n",
      "         468       0.00      0.00      0.00         1\n",
      "         469       0.00      0.00      0.00         0\n",
      "         470       1.00      0.50      0.67         2\n",
      "         471       0.00      0.00      0.00         2\n",
      "         472       1.00      0.33      0.50         3\n",
      "         473       0.00      0.00      0.00         1\n",
      "         474       0.00      0.00      0.00         0\n",
      "         475       0.00      0.00      0.00         0\n",
      "         476       0.00      0.00      0.00         3\n",
      "         477       1.00      0.67      0.80         6\n",
      "         478       0.00      0.00      0.00         3\n",
      "         479       0.00      0.00      0.00         2\n",
      "         480       1.00      1.00      1.00         1\n",
      "         481       0.00      0.00      0.00         3\n",
      "         482       0.00      0.00      0.00         1\n",
      "         483       0.00      0.00      0.00         2\n",
      "         484       0.00      0.00      0.00         2\n",
      "         485       0.00      0.00      0.00         4\n",
      "         486       0.00      0.00      0.00         0\n",
      "         487       0.00      0.00      0.00         3\n",
      "         488       1.00      0.33      0.50         3\n",
      "         489       0.00      0.00      0.00         0\n",
      "         490       0.00      0.00      0.00         2\n",
      "         491       0.00      0.00      0.00         0\n",
      "         492       0.00      0.00      0.00         0\n",
      "         493       0.00      0.00      0.00         2\n",
      "         494       0.00      0.00      0.00         3\n",
      "         495       0.00      0.00      0.00         5\n",
      "         496       0.00      0.00      0.00         2\n",
      "         497       0.00      0.00      0.00         3\n",
      "         498       0.33      1.00      0.50         1\n",
      "         499       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.65      0.29      0.40      3662\n",
      "   macro avg       0.30      0.17      0.20      3662\n",
      "weighted avg       0.54      0.29      0.36      3662\n",
      " samples avg       0.39      0.29      0.31      3662\n",
      "\n",
      "Time taken to run this cell : 0:00:43.372154\n"
     ]
    }
   ],
   "source": [
    "#print(grid.best_params_['estimator__alpha'])\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log', alpha=grid.best_params_['estimator__alpha'], penalty='l1'), n_jobs=-2)\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict (x_test_multilabel)\n",
    "\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------+--------------------+----------+\n",
      "|           Model name           |   Parameter   |      f1_score      | Accuracy |\n",
      "+--------------------------------+---------------+--------------------+----------+\n",
      "|      Logistic Regression       |  alpha = 1e-3 | 0.4029400678477196 |  0.178   |\n",
      "| Logistic Regression after grid | alpha = 1e-05 | 0.4021184036315491 |  0.181   |\n",
      "+--------------------------------+---------------+--------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "x.add_row([\"Logistic Regression after grid\",\"alpha = \"+str(grid.best_params_['estimator__alpha']), f1, str(metrics.accuracy_score(y_test, predictions))])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.165\n",
      "Hamming loss  0.0034\n",
      "Macro-average quality numbers\n",
      "Precision: 0.3022, Recall: 0.2152, F1-measure: 0.2291\n",
      "Micro-average quality numbers\n",
      "Precision: 0.5583, Recall: 0.3427, F1-measure: 0.4247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.31      0.32        97\n",
      "           1       0.69      0.51      0.58       140\n",
      "           2       0.58      0.40      0.47       125\n",
      "           3       0.78      0.71      0.75        91\n",
      "           4       0.72      0.55      0.62        88\n",
      "           5       0.70      0.47      0.56       136\n",
      "           6       0.09      0.12      0.10        25\n",
      "           7       0.50      0.44      0.47        27\n",
      "           8       0.85      0.59      0.70        88\n",
      "           9       0.69      0.47      0.56        81\n",
      "          10       0.70      0.57      0.63        61\n",
      "          11       0.74      0.76      0.75        82\n",
      "          12       0.33      0.25      0.29        40\n",
      "          13       0.05      0.03      0.04        34\n",
      "          14       0.48      0.17      0.25        89\n",
      "          15       0.40      0.39      0.40        46\n",
      "          16       0.57      0.51      0.54        39\n",
      "          17       0.38      0.27      0.32        22\n",
      "          18       0.46      0.28      0.35        57\n",
      "          19       0.40      0.27      0.32        45\n",
      "          20       0.35      0.33      0.34        21\n",
      "          21       0.65      0.59      0.62        56\n",
      "          22       0.53      0.50      0.51        18\n",
      "          23       0.52      0.37      0.43        30\n",
      "          24       0.50      0.36      0.42        11\n",
      "          25       0.38      0.21      0.27        29\n",
      "          26       0.12      0.11      0.12         9\n",
      "          27       0.62      0.33      0.43        39\n",
      "          28       0.50      0.56      0.53         9\n",
      "          29       0.70      0.88      0.78        16\n",
      "          30       0.89      0.35      0.50        23\n",
      "          31       0.55      0.21      0.30        29\n",
      "          32       0.82      0.67      0.74        21\n",
      "          33       0.50      0.17      0.25         6\n",
      "          34       0.89      0.87      0.88        45\n",
      "          35       0.29      0.14      0.19        29\n",
      "          36       0.80      0.40      0.53        10\n",
      "          37       0.33      0.12      0.18         8\n",
      "          38       0.60      0.38      0.46         8\n",
      "          39       0.50      0.06      0.11        17\n",
      "          40       1.00      0.09      0.17        11\n",
      "          41       0.40      0.22      0.29         9\n",
      "          42       0.50      0.29      0.36         7\n",
      "          43       0.75      0.75      0.75         4\n",
      "          44       1.00      0.29      0.44         7\n",
      "          45       0.00      0.00      0.00         7\n",
      "          46       0.69      0.32      0.44        28\n",
      "          47       0.00      0.00      0.00         2\n",
      "          48       0.40      0.29      0.33         7\n",
      "          49       0.40      0.29      0.33         7\n",
      "          50       1.00      0.70      0.82        10\n",
      "          51       0.57      0.36      0.44        11\n",
      "          52       0.58      0.58      0.58        12\n",
      "          53       0.00      0.00      0.00         6\n",
      "          54       0.88      0.78      0.82         9\n",
      "          55       0.00      0.00      0.00         0\n",
      "          56       0.38      0.43      0.40         7\n",
      "          57       0.27      0.21      0.24        14\n",
      "          58       0.75      0.10      0.18        29\n",
      "          59       1.00      0.33      0.50        18\n",
      "          60       0.60      0.23      0.33        13\n",
      "          61       1.00      0.20      0.33         5\n",
      "          62       0.80      0.36      0.50        11\n",
      "          63       0.00      0.00      0.00         6\n",
      "          64       1.00      0.41      0.58        17\n",
      "          65       0.45      0.45      0.45        11\n",
      "          66       0.00      0.00      0.00        14\n",
      "          67       0.62      0.56      0.59         9\n",
      "          68       0.50      0.14      0.22        14\n",
      "          69       0.73      0.44      0.55        18\n",
      "          70       0.50      0.08      0.13        13\n",
      "          71       0.75      0.20      0.32        15\n",
      "          72       0.80      0.36      0.50        11\n",
      "          73       0.67      0.22      0.33         9\n",
      "          74       0.38      0.33      0.35         9\n",
      "          75       0.00      0.00      0.00         2\n",
      "          76       0.20      0.04      0.07        23\n",
      "          77       0.75      0.43      0.55         7\n",
      "          78       0.00      0.00      0.00         2\n",
      "          79       0.67      0.29      0.40         7\n",
      "          80       1.00      0.62      0.77         8\n",
      "          81       1.00      0.60      0.75        10\n",
      "          82       0.73      0.92      0.81        12\n",
      "          83       0.67      0.33      0.44         6\n",
      "          84       0.00      0.00      0.00         2\n",
      "          85       0.00      0.00      0.00         9\n",
      "          86       0.50      0.33      0.40         6\n",
      "          87       0.00      0.00      0.00         4\n",
      "          88       0.00      0.00      0.00         2\n",
      "          89       0.80      1.00      0.89         8\n",
      "          90       0.40      0.25      0.31         8\n",
      "          91       0.33      0.20      0.25         5\n",
      "          92       0.00      0.00      0.00         9\n",
      "          93       0.91      0.83      0.87        12\n",
      "          94       0.50      0.50      0.50         2\n",
      "          95       0.00      0.00      0.00         7\n",
      "          96       0.00      0.00      0.00        13\n",
      "          97       0.20      0.06      0.09        18\n",
      "          98       0.00      0.00      0.00         0\n",
      "          99       0.40      0.22      0.29         9\n",
      "         100       0.50      0.17      0.25         6\n",
      "         101       0.50      0.30      0.37        10\n",
      "         102       0.17      0.08      0.11        12\n",
      "         103       0.50      0.50      0.50         4\n",
      "         104       1.00      0.30      0.46        10\n",
      "         105       0.00      0.00      0.00         2\n",
      "         106       1.00      0.25      0.40         8\n",
      "         107       0.00      0.00      0.00        10\n",
      "         108       0.33      0.20      0.25        10\n",
      "         109       1.00      0.50      0.67         2\n",
      "         110       1.00      0.86      0.92         7\n",
      "         111       1.00      0.06      0.12        16\n",
      "         112       0.00      0.00      0.00         6\n",
      "         113       0.80      0.50      0.62         8\n",
      "         114       0.00      0.00      0.00         4\n",
      "         115       0.20      0.50      0.29         2\n",
      "         116       0.89      0.73      0.80        11\n",
      "         117       0.83      1.00      0.91         5\n",
      "         118       0.67      0.29      0.40         7\n",
      "         119       1.00      0.67      0.80        15\n",
      "         120       0.75      0.75      0.75         4\n",
      "         121       0.83      0.36      0.50        14\n",
      "         122       0.00      0.00      0.00         4\n",
      "         123       0.67      0.40      0.50         5\n",
      "         124       0.00      0.00      0.00         2\n",
      "         125       1.00      0.29      0.44         7\n",
      "         126       0.67      0.67      0.67         3\n",
      "         127       0.00      0.00      0.00         7\n",
      "         128       1.00      0.83      0.91         6\n",
      "         129       0.00      0.00      0.00         3\n",
      "         130       0.00      0.00      0.00         3\n",
      "         131       0.50      0.10      0.17        10\n",
      "         132       1.00      0.50      0.67         8\n",
      "         133       0.00      0.00      0.00         2\n",
      "         134       0.00      0.00      0.00         2\n",
      "         135       0.00      0.00      0.00         8\n",
      "         136       0.00      0.00      0.00         2\n",
      "         137       0.00      0.00      0.00         5\n",
      "         138       0.00      0.00      0.00         4\n",
      "         139       0.50      0.50      0.50         4\n",
      "         140       0.00      0.00      0.00         3\n",
      "         141       0.00      0.00      0.00         3\n",
      "         142       0.00      0.00      0.00         3\n",
      "         143       0.00      0.00      0.00         1\n",
      "         144       0.29      0.50      0.36         4\n",
      "         145       0.00      0.00      0.00         1\n",
      "         146       0.00      0.00      0.00         2\n",
      "         147       0.50      0.25      0.33         4\n",
      "         148       0.50      0.17      0.25         6\n",
      "         149       0.40      0.50      0.44         4\n",
      "         150       0.00      0.00      0.00         3\n",
      "         151       0.00      0.00      0.00         2\n",
      "         152       1.00      0.14      0.25         7\n",
      "         153       0.00      0.00      0.00         7\n",
      "         154       0.00      0.00      0.00         4\n",
      "         155       1.00      0.33      0.50         6\n",
      "         156       0.00      0.00      0.00         6\n",
      "         157       0.86      0.86      0.86         7\n",
      "         158       1.00      0.50      0.67         4\n",
      "         159       0.00      0.00      0.00         2\n",
      "         160       0.00      0.00      0.00         1\n",
      "         161       1.00      0.11      0.20         9\n",
      "         162       1.00      0.25      0.40         4\n",
      "         163       0.33      1.00      0.50         1\n",
      "         164       0.33      0.25      0.29         4\n",
      "         165       0.00      0.00      0.00         7\n",
      "         166       0.25      0.50      0.33         2\n",
      "         167       1.00      0.33      0.50         6\n",
      "         168       0.00      0.00      0.00         1\n",
      "         169       1.00      0.33      0.50         6\n",
      "         170       1.00      0.50      0.67         4\n",
      "         171       0.50      0.25      0.33         4\n",
      "         172       1.00      0.43      0.60         7\n",
      "         173       0.50      1.00      0.67         2\n",
      "         174       0.00      0.00      0.00         6\n",
      "         175       0.00      0.00      0.00         9\n",
      "         176       0.67      0.67      0.67         3\n",
      "         177       1.00      1.00      1.00         2\n",
      "         178       0.00      0.00      0.00         2\n",
      "         179       0.67      0.33      0.44         6\n",
      "         180       1.00      0.50      0.67         4\n",
      "         181       0.83      0.50      0.62        10\n",
      "         182       0.50      0.07      0.12        15\n",
      "         183       0.00      0.00      0.00         2\n",
      "         184       0.50      0.40      0.44         5\n",
      "         185       0.00      0.00      0.00         5\n",
      "         186       0.00      0.00      0.00         2\n",
      "         187       0.25      0.33      0.29         3\n",
      "         188       1.00      0.50      0.67         8\n",
      "         189       0.00      0.00      0.00         5\n",
      "         190       1.00      0.20      0.33         5\n",
      "         191       1.00      0.78      0.88         9\n",
      "         192       0.25      0.33      0.29         3\n",
      "         193       0.33      1.00      0.50         1\n",
      "         194       0.00      0.00      0.00         1\n",
      "         195       0.00      0.00      0.00         1\n",
      "         196       0.00      0.00      0.00         4\n",
      "         197       0.00      0.00      0.00         1\n",
      "         198       0.00      0.00      0.00         7\n",
      "         199       0.00      0.00      0.00         5\n",
      "         200       0.00      0.00      0.00         7\n",
      "         201       0.00      0.00      0.00         2\n",
      "         202       0.00      0.00      0.00         4\n",
      "         203       0.50      0.20      0.29         5\n",
      "         204       1.00      0.12      0.22         8\n",
      "         205       0.00      0.00      0.00         2\n",
      "         206       0.50      0.33      0.40         3\n",
      "         207       0.00      0.00      0.00         2\n",
      "         208       0.50      0.50      0.50         2\n",
      "         209       0.00      0.00      0.00         6\n",
      "         210       0.00      0.00      0.00         5\n",
      "         211       0.00      0.00      0.00         1\n",
      "         212       0.00      0.00      0.00         2\n",
      "         213       0.50      0.50      0.50         4\n",
      "         214       0.00      0.00      0.00         5\n",
      "         215       0.00      0.00      0.00         3\n",
      "         216       0.00      0.00      0.00         5\n",
      "         217       0.00      0.00      0.00         1\n",
      "         218       0.75      0.60      0.67         5\n",
      "         219       0.00      0.00      0.00         3\n",
      "         220       0.00      0.00      0.00         5\n",
      "         221       1.00      0.14      0.25         7\n",
      "         222       0.00      0.00      0.00         5\n",
      "         223       0.00      0.00      0.00         2\n",
      "         224       0.00      0.00      0.00         4\n",
      "         225       0.00      0.00      0.00         3\n",
      "         226       0.33      0.25      0.29         4\n",
      "         227       0.00      0.00      0.00         1\n",
      "         228       1.00      0.25      0.40         4\n",
      "         229       0.00      0.00      0.00         1\n",
      "         230       0.00      0.00      0.00         9\n",
      "         231       0.50      0.22      0.31         9\n",
      "         232       1.00      0.38      0.55         8\n",
      "         233       1.00      1.00      1.00         1\n",
      "         234       0.00      0.00      0.00         3\n",
      "         235       0.00      0.00      0.00         4\n",
      "         236       1.00      0.25      0.40         4\n",
      "         237       0.00      0.00      0.00         2\n",
      "         238       0.86      0.86      0.86         7\n",
      "         239       0.00      0.00      0.00         0\n",
      "         240       0.40      0.33      0.36         6\n",
      "         241       1.00      1.00      1.00         1\n",
      "         242       0.00      0.00      0.00         0\n",
      "         243       0.50      1.00      0.67         1\n",
      "         244       0.00      0.00      0.00         2\n",
      "         245       0.00      0.00      0.00         1\n",
      "         246       0.50      0.67      0.57         3\n",
      "         247       0.00      0.00      0.00         3\n",
      "         248       1.00      0.33      0.50         3\n",
      "         249       0.00      0.00      0.00         0\n",
      "         250       0.00      0.00      0.00         6\n",
      "         251       1.00      0.40      0.57         5\n",
      "         252       0.20      0.20      0.20         5\n",
      "         253       0.33      0.33      0.33         3\n",
      "         254       0.00      0.00      0.00         3\n",
      "         255       0.00      0.00      0.00         5\n",
      "         256       0.00      0.00      0.00         1\n",
      "         257       0.00      0.00      0.00         3\n",
      "         258       1.00      0.50      0.67         4\n",
      "         259       0.00      0.00      0.00         4\n",
      "         260       0.00      0.00      0.00         5\n",
      "         261       0.00      0.00      0.00         0\n",
      "         262       0.33      0.67      0.44         3\n",
      "         263       0.00      0.00      0.00         5\n",
      "         264       0.00      0.00      0.00         5\n",
      "         265       0.00      0.00      0.00         2\n",
      "         266       0.00      0.00      0.00         2\n",
      "         267       0.50      1.00      0.67         1\n",
      "         268       0.00      0.00      0.00         1\n",
      "         269       1.00      0.50      0.67         2\n",
      "         270       1.00      0.33      0.50         3\n",
      "         271       1.00      1.00      1.00         1\n",
      "         272       0.50      0.25      0.33         4\n",
      "         273       0.50      0.10      0.17        10\n",
      "         274       0.00      0.00      0.00         3\n",
      "         275       1.00      0.25      0.40         4\n",
      "         276       0.00      0.00      0.00         0\n",
      "         277       0.00      0.00      0.00         1\n",
      "         278       0.00      0.00      0.00         3\n",
      "         279       0.00      0.00      0.00         7\n",
      "         280       0.00      0.00      0.00         2\n",
      "         281       0.00      0.00      0.00         1\n",
      "         282       0.25      0.50      0.33         2\n",
      "         283       0.00      0.00      0.00         1\n",
      "         284       0.00      0.00      0.00         2\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.00      0.00      0.00         1\n",
      "         287       0.00      0.00      0.00         4\n",
      "         288       0.00      0.00      0.00         0\n",
      "         289       0.00      0.00      0.00         6\n",
      "         290       0.00      0.00      0.00         1\n",
      "         291       0.75      0.43      0.55         7\n",
      "         292       0.00      0.00      0.00         3\n",
      "         293       0.00      0.00      0.00         2\n",
      "         294       0.00      0.00      0.00         0\n",
      "         295       0.00      0.00      0.00         1\n",
      "         296       0.00      0.00      0.00         3\n",
      "         297       0.00      0.00      0.00         4\n",
      "         298       0.00      0.00      0.00         2\n",
      "         299       0.00      0.00      0.00         0\n",
      "         300       0.33      1.00      0.50         1\n",
      "         301       0.00      0.00      0.00         4\n",
      "         302       0.00      0.00      0.00         0\n",
      "         303       0.00      0.00      0.00         0\n",
      "         304       0.40      0.33      0.36         6\n",
      "         305       0.00      0.00      0.00         3\n",
      "         306       0.00      0.00      0.00         0\n",
      "         307       0.00      0.00      0.00         4\n",
      "         308       1.00      0.50      0.67         2\n",
      "         309       0.00      0.00      0.00         3\n",
      "         310       0.00      0.00      0.00         1\n",
      "         311       0.00      0.00      0.00         2\n",
      "         312       1.00      0.50      0.67         2\n",
      "         313       0.00      0.00      0.00         4\n",
      "         314       0.00      0.00      0.00         2\n",
      "         315       0.00      0.00      0.00         0\n",
      "         316       0.71      0.71      0.71         7\n",
      "         317       0.50      1.00      0.67         1\n",
      "         318       0.33      0.50      0.40         2\n",
      "         319       0.00      0.00      0.00         3\n",
      "         320       1.00      0.50      0.67         6\n",
      "         321       0.00      0.00      0.00         2\n",
      "         322       0.00      0.00      0.00         1\n",
      "         323       0.00      0.00      0.00         4\n",
      "         324       0.00      0.00      0.00         2\n",
      "         325       0.00      0.00      0.00         1\n",
      "         326       0.00      0.00      0.00         2\n",
      "         327       0.50      0.50      0.50         2\n",
      "         328       0.20      0.50      0.29         2\n",
      "         329       0.00      0.00      0.00         5\n",
      "         330       1.00      0.33      0.50         3\n",
      "         331       0.00      0.00      0.00         3\n",
      "         332       0.00      0.00      0.00         2\n",
      "         333       0.00      0.00      0.00         1\n",
      "         334       0.67      0.50      0.57         4\n",
      "         335       1.00      0.67      0.80         3\n",
      "         336       0.00      0.00      0.00         1\n",
      "         337       1.00      0.50      0.67         2\n",
      "         338       0.00      0.00      0.00         2\n",
      "         339       0.00      0.00      0.00         1\n",
      "         340       0.00      0.00      0.00         0\n",
      "         341       0.00      0.00      0.00         1\n",
      "         342       0.00      0.00      0.00         3\n",
      "         343       0.00      0.00      0.00         9\n",
      "         344       0.00      0.00      0.00         1\n",
      "         345       1.00      1.00      1.00         1\n",
      "         346       0.00      0.00      0.00         1\n",
      "         347       0.00      0.00      0.00         0\n",
      "         348       0.50      1.00      0.67         1\n",
      "         349       1.00      0.33      0.50         3\n",
      "         350       0.00      0.00      0.00         4\n",
      "         351       0.00      0.00      0.00         4\n",
      "         352       1.00      0.67      0.80         6\n",
      "         353       0.00      0.00      0.00         0\n",
      "         354       0.00      0.00      0.00         6\n",
      "         355       0.00      0.00      0.00         5\n",
      "         356       0.00      0.00      0.00         2\n",
      "         357       0.57      1.00      0.73         4\n",
      "         358       0.00      0.00      0.00         0\n",
      "         359       0.00      0.00      0.00         1\n",
      "         360       0.00      0.00      0.00         1\n",
      "         361       0.00      0.00      0.00         0\n",
      "         362       0.00      0.00      0.00         1\n",
      "         363       0.00      0.00      0.00         1\n",
      "         364       0.00      0.00      0.00         1\n",
      "         365       0.50      1.00      0.67         1\n",
      "         366       0.00      0.00      0.00         4\n",
      "         367       1.00      0.33      0.50         3\n",
      "         368       0.00      0.00      0.00         2\n",
      "         369       0.50      0.50      0.50         4\n",
      "         370       0.00      0.00      0.00         2\n",
      "         371       0.43      0.50      0.46         6\n",
      "         372       0.00      0.00      0.00         4\n",
      "         373       0.00      0.00      0.00         0\n",
      "         374       1.00      0.50      0.67         4\n",
      "         375       0.00      0.00      0.00         3\n",
      "         376       0.00      0.00      0.00         1\n",
      "         377       0.00      0.00      0.00         4\n",
      "         378       1.00      0.75      0.86         4\n",
      "         379       0.00      0.00      0.00         4\n",
      "         380       0.00      0.00      0.00         3\n",
      "         381       0.00      0.00      0.00         1\n",
      "         382       0.00      0.00      0.00         2\n",
      "         383       1.00      1.00      1.00         2\n",
      "         384       1.00      1.00      1.00         1\n",
      "         385       0.00      0.00      0.00         0\n",
      "         386       0.00      0.00      0.00         3\n",
      "         387       0.00      0.00      0.00         0\n",
      "         388       0.00      0.00      0.00         3\n",
      "         389       0.00      0.00      0.00         2\n",
      "         390       0.00      0.00      0.00         2\n",
      "         391       0.00      0.00      0.00         3\n",
      "         392       0.14      0.20      0.17         5\n",
      "         393       0.00      0.00      0.00         1\n",
      "         394       0.00      0.00      0.00         3\n",
      "         395       0.00      0.00      0.00         1\n",
      "         396       0.00      0.00      0.00         1\n",
      "         397       0.00      0.00      0.00         4\n",
      "         398       0.00      0.00      0.00         3\n",
      "         399       0.00      0.00      0.00         2\n",
      "         400       0.50      0.33      0.40         3\n",
      "         401       0.00      0.00      0.00         1\n",
      "         402       0.00      0.00      0.00         1\n",
      "         403       0.00      0.00      0.00         1\n",
      "         404       0.00      0.00      0.00         0\n",
      "         405       1.00      1.00      1.00         1\n",
      "         406       0.00      0.00      0.00         3\n",
      "         407       1.00      0.50      0.67         2\n",
      "         408       0.50      0.50      0.50         2\n",
      "         409       0.00      0.00      0.00         0\n",
      "         410       0.00      0.00      0.00         0\n",
      "         411       0.00      0.00      0.00         6\n",
      "         412       0.00      0.00      0.00         5\n",
      "         413       0.00      0.00      0.00         3\n",
      "         414       0.00      0.00      0.00         0\n",
      "         415       1.00      0.50      0.67         2\n",
      "         416       0.00      0.00      0.00         0\n",
      "         417       0.00      0.00      0.00         1\n",
      "         418       0.00      0.00      0.00         0\n",
      "         419       0.00      0.00      0.00         3\n",
      "         420       0.00      0.00      0.00         2\n",
      "         421       0.00      0.00      0.00         2\n",
      "         422       0.00      0.00      0.00         1\n",
      "         423       0.00      0.00      0.00         0\n",
      "         424       0.00      0.00      0.00         1\n",
      "         425       0.00      0.00      0.00         0\n",
      "         426       0.20      1.00      0.33         1\n",
      "         427       0.00      0.00      0.00         2\n",
      "         428       0.00      0.00      0.00         0\n",
      "         429       0.00      0.00      0.00         0\n",
      "         430       0.00      0.00      0.00         2\n",
      "         431       0.00      0.00      0.00         0\n",
      "         432       0.00      0.00      0.00         0\n",
      "         433       0.00      0.00      0.00         1\n",
      "         434       0.00      0.00      0.00         1\n",
      "         435       0.00      0.00      0.00         3\n",
      "         436       0.33      1.00      0.50         2\n",
      "         437       0.00      0.00      0.00         1\n",
      "         438       1.00      0.50      0.67         2\n",
      "         439       1.00      0.33      0.50         3\n",
      "         440       0.00      0.00      0.00         2\n",
      "         441       0.00      0.00      0.00         1\n",
      "         442       0.50      1.00      0.67         1\n",
      "         443       0.25      1.00      0.40         1\n",
      "         444       0.00      0.00      0.00         1\n",
      "         445       0.00      0.00      0.00         4\n",
      "         446       0.00      0.00      0.00         5\n",
      "         447       0.00      0.00      0.00         3\n",
      "         448       0.00      0.00      0.00         0\n",
      "         449       0.00      0.00      0.00         0\n",
      "         450       0.00      0.00      0.00         0\n",
      "         451       0.00      0.00      0.00         2\n",
      "         452       0.00      0.00      0.00         4\n",
      "         453       0.00      0.00      0.00         0\n",
      "         454       0.00      0.00      0.00         0\n",
      "         455       1.00      0.75      0.86         4\n",
      "         456       0.00      0.00      0.00         0\n",
      "         457       0.00      0.00      0.00         0\n",
      "         458       0.00      0.00      0.00         3\n",
      "         459       1.00      1.00      1.00         1\n",
      "         460       0.00      0.00      0.00         2\n",
      "         461       1.00      1.00      1.00         1\n",
      "         462       0.00      0.00      0.00         2\n",
      "         463       0.00      0.00      0.00         0\n",
      "         464       0.00      0.00      0.00         0\n",
      "         465       0.00      0.00      0.00         2\n",
      "         466       0.00      0.00      0.00         3\n",
      "         467       0.00      0.00      0.00         2\n",
      "         468       0.00      0.00      0.00         1\n",
      "         469       0.00      0.00      0.00         0\n",
      "         470       1.00      0.50      0.67         2\n",
      "         471       0.00      0.00      0.00         2\n",
      "         472       0.50      0.33      0.40         3\n",
      "         473       0.00      0.00      0.00         1\n",
      "         474       0.00      0.00      0.00         0\n",
      "         475       0.00      0.00      0.00         0\n",
      "         476       0.00      0.00      0.00         3\n",
      "         477       1.00      1.00      1.00         6\n",
      "         478       0.00      0.00      0.00         3\n",
      "         479       0.00      0.00      0.00         2\n",
      "         480       1.00      1.00      1.00         1\n",
      "         481       0.00      0.00      0.00         3\n",
      "         482       0.00      0.00      0.00         1\n",
      "         483       0.33      0.50      0.40         2\n",
      "         484       0.00      0.00      0.00         2\n",
      "         485       0.00      0.00      0.00         4\n",
      "         486       0.00      0.00      0.00         0\n",
      "         487       0.00      0.00      0.00         3\n",
      "         488       1.00      0.33      0.50         3\n",
      "         489       0.00      0.00      0.00         0\n",
      "         490       0.00      0.00      0.00         2\n",
      "         491       0.00      0.00      0.00         0\n",
      "         492       0.00      0.00      0.00         0\n",
      "         493       1.00      0.50      0.67         2\n",
      "         494       0.00      0.00      0.00         3\n",
      "         495       0.00      0.00      0.00         5\n",
      "         496       0.00      0.00      0.00         2\n",
      "         497       0.50      0.33      0.40         3\n",
      "         498       0.33      1.00      0.50         1\n",
      "         499       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.56      0.34      0.42      3662\n",
      "   macro avg       0.30      0.22      0.23      3662\n",
      "weighted avg       0.51      0.34      0.39      3662\n",
      " samples avg       0.41      0.34      0.35      3662\n",
      "\n",
      "Time taken to run this cell : 0:00:02.541105\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "classifiersvm = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.00001, penalty='l1'), n_jobs=-2)\n",
    "classifiersvm.fit(x_train_multilabel, y_train)\n",
    "predictions = classifiersvm.predict(x_test_multilabel)\n",
    "\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------+--------------------+----------+\n",
      "|           Model name           |   Parameter   |      f1_score      | Accuracy |\n",
      "+--------------------------------+---------------+--------------------+----------+\n",
      "|      Logistic Regression       |  alpha = 1e-3 | 0.4029400678477196 |  0.178   |\n",
      "| Logistic Regression after grid | alpha = 1e-05 | 0.4021184036315491 |  0.181   |\n",
      "|           Linear SVM           | alpha = 1e-05 | 0.4247038917089678 |  0.165   |\n",
      "+--------------------------------+---------------+--------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "x.add_row([\"Linear SVM\",\"alpha = \"+str(grid.best_params_['estimator__alpha']), f1, str(metrics.accuracy_score(y_test, predictions))])\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SO_Tag_Predictor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
